{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583ec0b4",
   "metadata": {},
   "source": [
    "### 02_SQLite_IO_and_Batching\n",
    "\n",
    "Este bloque crea utilidades para:\n",
    "- Conectarse a SQLite con **WAL** y `synchronous=NORMAL`.\n",
    "- Crear tablas **_01_reddit_posts_preprocessed**, **_01_reddit_comments_preprocessed**, **_01_news_articles_preprocessed**.\n",
    "- Procesar texto en **lotes** con **checkpoint** (`_etl_checkpoints`) para reanudar.\n",
    "- Hacer un **smoke test** que migra N filas por tabla.\n",
    "\n",
    "**Notas de modelado**\n",
    "- Las tablas preprocesadas **no duplican** los metadatos crudos (p. ej. `score`, `url` de post), solo almacenan salidas de NLP + trazabilidad mÃ­nima.\n",
    "- `lang/lang_conf`: se detectan sobre el **texto combinado** (p. ej., `title + body`).\n",
    "- `content_hash`: hash estable del `combined_ml` para *dedup*/auditorÃ­a futura.\n",
    "- PK de preprocesadas = PK de crudas + `FOREIGN KEY ... ON DELETE CASCADE`, asÃ­ mantienes integridad referencial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12193c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from utilities_text_processing import text_preprocessing, text_preprocessing_llm, text_preprocessing_ml\n",
    "\n",
    "# Default route\n",
    "DB_PATH_DEFAULT = os.path.expanduser(\n",
    "    \"~/Desktop/all_folders/Investings_project/app/data/stock_data.db\"\n",
    ")\n",
    "\n",
    "PROC_VERSION = \"01_preprocess_v0.1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "290aebe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DB_PATH_DEFAULT] /home/aprohack/Desktop/all_folders/Investings_project/app/data/stock_data.db\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# chatGPT part\n",
    "\n",
    "def guess_runtime_dir() -> Path:\n",
    "    \"\"\"\n",
    "    Devuelve el directorio 'base' desde el cual debemos resolver rutas relativas.\n",
    "    - Si el cÃ³digo corre como script (.py): usa la carpeta de ese archivo (__file__).\n",
    "    - Si corre en notebook (no existe __file__): usa el directorio de trabajo actual (Path.cwd()).\n",
    "      En Jupyter/Lab esto suele ser la carpeta donde estÃ¡ el .ipynb, salvo que hayas cambiado el cwd.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Modo script / mÃ³dulo\n",
    "        return Path(__file__).resolve().parent\n",
    "    except NameError:\n",
    "        # Modo notebook / consola interactiva\n",
    "        return Path.cwd().resolve()\n",
    "\n",
    "\n",
    "def find_project_root(start: Path = None,\n",
    "                      markers = (\"app.py\", \"Dockerfile\", \"requirements.txt\", \"python_scripts\", \"data\")) -> Path:\n",
    "    \"\"\"\n",
    "    Sube por la jerarquÃ­a de carpetas hasta encontrar un directorio que contenga\n",
    "    al menos uno de los 'markers' tÃ­picos de tu proyecto (p. ej., app.py, data/).\n",
    "    Si no encuentra un candidato, devuelve 'start'.\n",
    "    \"\"\"\n",
    "    start = (start or guess_runtime_dir()).resolve()\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if any((parent / m).exists() for m in markers):\n",
    "            return parent\n",
    "    return start\n",
    "\n",
    "\n",
    "def resolve_db_path(db_relative: str = \"data/stock_data.db\",\n",
    "                    create_dirs: bool = False,\n",
    "                    env_var: str = \"INVESTINGS_DB_PATH\") -> Path:\n",
    "    \"\"\"\n",
    "    Resuelve la ruta absoluta de la DB, con esta prioridad:\n",
    "    1) Variable de entorno INVESTINGS_DB_PATH (si estÃ¡ definida).\n",
    "    2) Si 'db_relative' es absoluto, Ãºsalo tal cual.\n",
    "    3) Une el 'project_root' (descubierto) con 'db_relative' (por defecto data/stock_data.db).\n",
    "\n",
    "    ParÃ¡metros:\n",
    "    - db_relative: ruta relativa a la raÃ­z del proyecto (o absoluta si quieres forzar).\n",
    "    - create_dirs: si True, crea la carpeta padre de la DB si no existe.\n",
    "    - env_var: variable de entorno para sobre-escribir la ruta cuando corras en otras mÃ¡quinas.\n",
    "\n",
    "    Devuelve:\n",
    "    - Path absoluto a la base de datos.\n",
    "    \"\"\"\n",
    "    # 1) Override por variable de entorno\n",
    "    env = os.getenv(env_var)\n",
    "    if env:\n",
    "        p = Path(os.path.expanduser(env)).resolve()\n",
    "        if create_dirs:\n",
    "            p.parent.mkdir(parents=True, exist_ok=True)\n",
    "        return p\n",
    "\n",
    "    # 2) Â¿db_relative ya es absoluto?\n",
    "    p = Path(db_relative).expanduser()\n",
    "    if p.is_absolute():\n",
    "        if create_dirs:\n",
    "            p.parent.mkdir(parents=True, exist_ok=True)\n",
    "        return p.resolve()\n",
    "\n",
    "    # 3) Resolver relativo a la raÃ­z del proyecto\n",
    "    root = find_project_root()\n",
    "    full = (root / db_relative).resolve()\n",
    "    if create_dirs:\n",
    "        full.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return full\n",
    "\n",
    "\n",
    "# ðŸ‘‰ Define/actualiza el valor por defecto que usa el resto del notebook:\n",
    "DB_PATH_DEFAULT = str(resolve_db_path(\"data/stock_data.db\"))\n",
    "print(\"[DB_PATH_DEFAULT]\", DB_PATH_DEFAULT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d323516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'foreign_keys': 1, 'journal_mode': 'wal', 'synchronous': 1}\n"
     ]
    }
   ],
   "source": [
    "def get_connection(db_path: Optional[str] = None) -> sqlite3.Connection:\n",
    "    '''\n",
    "    Opens a connection to SQLite and appies PRAGMAs for efficiency\n",
    "    foreign_keys=ON (integrity)\n",
    "    journal_mode=WAL (better concurrency)\n",
    "    synchronous=NORMAL (decent efficiency with WAL)\n",
    "    '''\n",
    "    if db_path is None:\n",
    "        path = resolve_db_path(\"data/stock_data.db\")\n",
    "    else:\n",
    "        # Permite pasar absoluta o relativa; si es relativa la resuelve desde el root del proyecto\n",
    "        path = resolve_db_path(db_path)\n",
    "    con = sqlite3.connect(str(path))\n",
    "    con.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "    mode = con.execute(\"PRAGMA journal_mode;\").fetchone()[0]\n",
    "    if mode.upper() != \"WAL\":\n",
    "        con.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "    con.execute(\"PRAGMA synchronous=NORMAL;\")\n",
    "    return con\n",
    "\n",
    "def check_pragmas(con: sqlite3.Connection) -> Dict[str, str]:\n",
    "    return {\n",
    "        'foreign_keys': con.execute(\"PRAGMA foreign_keys;\").fetchone()[0],\n",
    "        \"journal_mode\": con.execute(\"PRAGMA journal_mode;\").fetchone()[0],\n",
    "        \"synchronous\": con.execute(\"PRAGMA synchronous;\").fetchone()[0],\n",
    "    }\n",
    "\n",
    "con = get_connection()\n",
    "print(check_pragmas(con))\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb7999",
   "metadata": {},
   "source": [
    "# Table schemes + Indices + Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe13e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DDL_PREPROCESSED = {\n",
    "    \"_01_reddit_posts_preprocessed\": \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS _01_reddit_posts_preprocessed (\n",
    "        post_id TEXT PRIMARY KEY,\n",
    "        lang TEXT,\n",
    "        lang_conf REAL,\n",
    "        combined_raw TEXT,        -- title + body\n",
    "        combined_ml TEXT,\n",
    "        combined_llm TEXT,\n",
    "        content_hash TEXT,         -- hash combined_ml normalized\n",
    "        processed_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "        proc_version TEXT,\n",
    "        FOREIGN KEY (post_id) REFERENCES reddit_posts (post_id) ON DELETE CASCADE\n",
    "    );\n",
    "    \"\"\",\n",
    "\n",
    "    \"_01_reddit_comments_preprocessed\": \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS _01_reddit_comments_preprocessed (\n",
    "        comment_id TEXT PRIMARY KEY,\n",
    "        post_id TEXT NOT NULL,\n",
    "        lang TEXT,\n",
    "        lang_conf REAL,\n",
    "        body_raw TEXT, \n",
    "        body_ml TEXT,\n",
    "        body_llm TEXT,\n",
    "        content_hash TEXT,\n",
    "        processed_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "        proc_version TEXT,\n",
    "        FOREIGN KEY (comment_id) REFERENCES reddit_comments (comment_id) ON DELETE CASCADE\n",
    "        FOREIGN KEY (post_id) REFERENCES reddit_posts (post_id) ON DELETE CASCADE\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"_01_news_articles_preprocessed\": \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS _01_news_articles_preprocessed (\n",
    "            url TEXT PRIMARY KEY,\n",
    "            lang TEXT,\n",
    "            lang_conf REAL,\n",
    "            combined_raw TEXT,     -- title + description + content\n",
    "            combined_ml TEXT,\n",
    "            combined_llm TEXT,\n",
    "            content_hash TEXT,\n",
    "            processed_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "            proc_version TEXT,\n",
    "            FOREIGN KEY (url) REFERENCES news_articles (url) ON DELETE CASCADE\n",
    "        );\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "DDL_INDEXES = {\n",
    "    \"_01_reddit_posts_preprocessed\": [\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_rpp_hash ON _01_reddit_posts_preprocessed (content_hash);\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_rpp_proc_at ON _01_reddit_posts_preprocessed (processed_at);\",\n",
    "    ],\n",
    "    \"_01_reddit_comments_preprocessed\": [\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_rcp_hash ON _01_reddit_comments_preprocessed (content_hash);\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_rcp_post ON _01_reddit_comments_preprocessed (post_id);\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_rcp_proc_at ON _01_reddit_comments_preprocessed (processed_at);\",\n",
    "    ],\n",
    "    \"_01_news_articles_preprocessed\": [\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_nap_hash ON _01_news_articles_preprocessed (content_hash);\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_nap_proc_at ON _01_news_articles_preprocessed (processed_at);\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "DDL_CHECKPOINTS = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS _etl_checkpoints (\n",
    "    target_table TEXT PRIMARY KEY,    -- p.ej. '_01_reddit_posts_preprocessed'\n",
    "    source_table TEXT NOT NULL,       -- p.ej. 'reddit_posts'\n",
    "    pk_col       TEXT NOT NULL,       -- p.ej. 'post_id'\n",
    "    time_col     TEXT NOT NULL,       -- p.ej. 'created_utc' | 'published_at'\n",
    "    last_pk      TEXT,                -- Ãºltimo PK procesado\n",
    "    last_time    REAL,                -- Ãºltimo timestamp procesado (segundos UNIX)\n",
    "    updated_at   DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Vistas de conveniencia (joins listos para usar)\n",
    "DDL_VIEWS = {\n",
    "    \"v_reddit_posts_enriched\": \"\"\"\n",
    "    CREATE VIEW IF NOT EXISTS v_reddit_posts_enriched AS\n",
    "    SELECT p.*, pp.lang, pp.lang_conf, pp.combined_ml, pp.combined_llm, pp.content_hash, pp.processed_at\n",
    "    FROM reddit_posts p\n",
    "    LEFT JOIN _01_reddit_posts_preprocessed pp ON pp.post_id = p.post_id;\n",
    "    \"\"\",\n",
    "    \"v_reddit_comments_enriched\": \"\"\"\n",
    "    CREATE VIEW IF NOT EXISTS v_reddit_comments_enriched AS\n",
    "    SELECT c.*, cp.lang, cp.lang_conf, cp.body_ml, cp.body_llm, cp.content_hash, cp.processed_at\n",
    "    FROM reddit_comments c\n",
    "    LEFT JOIN _01_reddit_comments_preprocessed cp ON cp.comment_id = c.comment_id;\n",
    "    \"\"\",\n",
    "    \"v_news_articles_enriched\": \"\"\"\n",
    "    CREATE VIEW IF NOT EXISTS v_news_articles_enriched AS\n",
    "    SELECT n.*, np.lang, np.lang_conf, np.combined_ml, np.combined_llm, np.content_hash, np.processed_at\n",
    "    FROM news_articles n\n",
    "    LEFT JOIN _01_news_articles_preprocessed np ON np.url = n.url;\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a706f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_processed_schema(db_path: Optional[str] = None):\n",
    "    con = get_connection(db_path)\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        for ddl in DDL_PREPROCESSED.values():\n",
    "            cur.execute(ddl)\n",
    "        for tbl, idxs in DDL_INDEXES.items():\n",
    "            for idx in idxs:\n",
    "                cur.execute(idx)\n",
    "        cur.execute(DDL_CHECKPOINTS)\n",
    "        for ddl in DDL_VIEWS.values():\n",
    "            cur.execute(ddl)\n",
    "        con.commit()\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "create_processed_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0722583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_str(x) -> str:\n",
    "    return \"\" if x is None else str(x)\n",
    "\n",
    "def compute_content_hash(text: str) -> str:\n",
    "    norm = (_ensure_str(text)).strip()\n",
    "    h = hashlib.blake2b(norm.encode('utf-8'), digest_size = 20)\n",
    "    return h.hexdigest()\n",
    "def combine_post_text(title: Optional[str], body: Optional[str]) -> str:\n",
    "    parts = [p.strip() for p in [title, body] if p and p.strip()]\n",
    "    return \" - \".join(parts) if parts else \"\"\n",
    "\n",
    "def combine_news_text(title: Optional[str], desc: Optional[str], content: Optional[str]) -> str:\n",
    "    parts = [p.strip() for p in [title, desc, content] if p and p.strip()]\n",
    "    return \" - \".join(parts) if parts else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b72b31bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_checkpoint(con: sqlite3.Connection,\n",
    "                      target_table: str,\n",
    "                      source_table: str,\n",
    "                      pk_col: str,\n",
    "                      time_col: str,\n",
    "                      last_pk: Optional[str],\n",
    "                      last_time: Optional[float]\n",
    "                      ) -> None:\n",
    "    con.execute(\"\"\"\n",
    "        INSERT INTO _etl_checkpoints(target_table, source_table, pk_col, time_col, last_pk, last_time, updated_at)\n",
    "        VALUES (?, ?, ?, ?, ? , ?, CURRENT_TIMESTAMP)\n",
    "        ON CONFLICT(target_table) DO UPDATE SET\n",
    "            last_pk = excluded.last_pk,\n",
    "            last_time = excluded.last_time,\n",
    "            updated_at = CURRENT_TIMESTAMP;\n",
    "        \"\"\", (target_table, source_table, pk_col, time_col, last_pk, last_time))\n",
    "    \n",
    "def get_checkpoint(con: sqlite3.Connection, target_table: str) -> Optional[Tuple[str, str, str, str, Optional[str], Optional[float]]]:\n",
    "    row = con.execute(\"\"\"\n",
    "                      SELECT target_table, source_table, pk_col, time_col, last_pk, last_time\n",
    "                      FROM _etl_checkpoints WHERE target_table = ?;\n",
    "                      \"\"\", (target_table, )).fetchone()\n",
    "    return row\n",
    "\n",
    "def _order_time_sql(source_table: str, time_col: str) -> str:\n",
    "    if source_table in (\"reddit_posts\", \"reddit_comments\") and time_col == \"created_utc\":\n",
    "        return f\"COALESCE({source_table}.created_utc, 0.0)\"\n",
    "    if source_table == \"news_articles\" and time_col == \"published_at\":\n",
    "        return (\n",
    "            \"COALESCE(\"\n",
    "            \"CAST(strftime('%s', news_articles.published_at) AS REAL), \"\n",
    "            \"CAST(strftime('%s', news_articles.fetch_date)   AS REAL), \"\n",
    "            \"0.0)\"\n",
    "        )\n",
    "    return f\"COALESCE(CAST(strftime('%s', {source_table}.{time_col}) AS REAL), 0.0)\"\n",
    "\n",
    "\n",
    "\n",
    "def fetch_unprocessed_batch(con: sqlite3.Connection,\n",
    "                            source_table: str,\n",
    "                            processed_table: str,\n",
    "                            pk_col: str,\n",
    "                            time_col: str,\n",
    "                            select_cols: List[str],\n",
    "                            batch_size: int,\n",
    "                            checkpoint: Optional[Tuple[str, str, str, str, Optional[str], Optional[float]]] = None):\n",
    "    '''\n",
    "    Obtains some crude rows that arent in the preprocessed table\n",
    "    Respects checkpoint: (last_time, last_pk) for sorting and continuing\n",
    "    '''\n",
    "\n",
    "    order_time = _order_time_sql(source_table, time_col)\n",
    "    last_pk = None\n",
    "    last_time = None\n",
    "    if checkpoint:\n",
    "        _, src_tbl, _, _, last_pk, last_time = checkpoint\n",
    "    \n",
    "    # Criteria: \"not exists\" + checkpoint per (time, pk) for an stable order\n",
    "    # if last_time/last_pk are NULL, the condition is ignored (first time)\n",
    "\n",
    "    where_checkpoint = f\"\"\"\n",
    "    AND (\n",
    "        (? IS NULL AND ? IS NULL)\n",
    "        OR\n",
    "        ({order_time} > ?)\n",
    "        OR\n",
    "        ({order_time} = ? AND {source_table}.{pk_col} > ?)\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    SELECT {\", \".join([f\"{source_table}.{c}\" for c in select_cols])}\n",
    "    FROM {source_table}\n",
    "    LEFT JOIN {processed_table} p ON p.{pk_col} = {source_table}.{pk_col}\n",
    "    WHERE p.{pk_col} IS NULL\n",
    "    {where_checkpoint}\n",
    "    ORDER BY {order_time} ASC, {source_table}.{pk_col} ASC\n",
    "    LIMIT ?;\n",
    "    \"\"\"\n",
    "    params = (last_time, last_pk, last_time, last_time, last_pk, batch_size)\n",
    "    cur = con.execute(sql, params)\n",
    "    rows = cur.fetchall()\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f43d4",
   "metadata": {},
   "source": [
    "# Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7d14bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_posts_processed(con: sqlite3.Connection, rows: List[Tuple]):\n",
    "    \"\"\"\n",
    "    Inserta/actualiza filas en _01_reddit_posts_preprocessed.\n",
    "    'rows' es una lista de tuplas con:\n",
    "      (post_id, lang, lang_conf, combined_raw, combined_ml, combined_llm, content_hash, PROC_VERSION)\n",
    "    \"\"\"\n",
    "    con.executemany(\"\"\"\n",
    "        INSERT INTO _01_reddit_posts_preprocessed(\n",
    "            post_id, lang, lang_conf, combined_raw, combined_ml, combined_llm, content_hash, proc_version\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ON CONFLICT(post_id) DO UPDATE SET\n",
    "            lang = excluded.lang,\n",
    "            lang_conf = excluded.lang_conf,\n",
    "            combined_raw = excluded.combined_raw,\n",
    "            combined_ml = excluded.combined_ml,\n",
    "            combined_llm = excluded.combined_llm,\n",
    "            content_hash = excluded.content_hash,\n",
    "            processed_at = CURRENT_TIMESTAMP,\n",
    "            proc_version = excluded.proc_version;\n",
    "    \"\"\", rows)\n",
    "\n",
    "def upsert_comments_processed(con: sqlite3.Connection, rows: List[Tuple]):\n",
    "    \"\"\"\n",
    "    Inserta/actualiza filas en _01_reddit_comments_preprocessed.\n",
    "    Tuplas:\n",
    "      (comment_id, post_id, lang, lang_conf, body_raw, body_ml, body_llm, content_hash, PROC_VERSION)\n",
    "    \"\"\"\n",
    "    con.executemany(\"\"\"\n",
    "        INSERT INTO _01_reddit_comments_preprocessed(\n",
    "            comment_id, post_id, lang, lang_conf, body_raw, body_ml, body_llm, content_hash, proc_version\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ON CONFLICT(comment_id) DO UPDATE SET\n",
    "            post_id = excluded.post_id,\n",
    "            lang = excluded.lang,\n",
    "            lang_conf = excluded.lang_conf,\n",
    "            body_raw = excluded.body_raw,\n",
    "            body_ml = excluded.body_ml,\n",
    "            body_llm = excluded.body_llm,\n",
    "            content_hash = excluded.content_hash,\n",
    "            processed_at = CURRENT_TIMESTAMP,\n",
    "            proc_version = excluded.proc_version;\n",
    "    \"\"\", rows)\n",
    "\n",
    "def upsert_news_processed(con: sqlite3.Connection, rows: List[Tuple]):\n",
    "    \"\"\"\n",
    "    Inserta/actualiza filas en _01_news_articles_preprocessed.\n",
    "    Tuplas:\n",
    "      (url, lang, lang_conf, combined_raw, combined_ml, combined_llm, content_hash, PROC_VERSION)\n",
    "    \"\"\"\n",
    "    con.executemany(\"\"\"\n",
    "        INSERT INTO _01_news_articles_preprocessed(\n",
    "            url, lang, lang_conf, combined_raw, combined_ml, combined_llm, content_hash, proc_version\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ON CONFLICT(url) DO UPDATE SET\n",
    "            lang = excluded.lang,\n",
    "            lang_conf = excluded.lang_conf,\n",
    "            combined_raw = excluded.combined_raw,\n",
    "            combined_ml = excluded.combined_ml,\n",
    "            combined_llm = excluded.combined_llm,\n",
    "            content_hash = excluded.content_hash,\n",
    "            processed_at = CURRENT_TIMESTAMP,\n",
    "            proc_version = excluded.proc_version;\n",
    "    \"\"\", rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d7a00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from click import Option\n",
    "\n",
    "\n",
    "def process_posts_batch(db_path: Optional[str] = None,\n",
    "                        batch_size: int = 100,\n",
    "                        to_lowercase_ml: bool = False) -> Tuple[int, Optional[str], Optional[float]]:\n",
    "    # Returns (n_inserted, last_pk, last_time)\n",
    "    con = get_connection(db_path)\n",
    "    try:\n",
    "        target = \"_01_reddit_posts_preprocessed\"\n",
    "        source = \"reddit_posts\"\n",
    "        pk_col = \"post_id\"\n",
    "        time_col = \"created_utc\"\n",
    "        select_cols = [\"post_id\", \"title\", \"body\", \"created_utc\"]\n",
    "\n",
    "        ckpt = get_checkpoint(con, target)\n",
    "\n",
    "        rows = fetch_unprocessed_batch(con, source, target, pk_col, time_col, select_cols, batch_size, ckpt)\n",
    "        if not rows:\n",
    "            return 0, None, None\n",
    "        \n",
    "        to_upsert = []\n",
    "        lsat_pk = None\n",
    "        last_time = None\n",
    "\n",
    "        for post_id, title, body, created_utc in rows:\n",
    "            combined_raw = combine_post_text(title, body)\n",
    "            ml = text_preprocessing_ml(combined_raw, to_lowercase_ml)\n",
    "            llm = text_preprocessing_llm(combined_raw, to_lowercase=False)\n",
    "\n",
    "            lang = ml[\"lang\"]\n",
    "            try:\n",
    "                lang_conf = float(ml['lang_conf'])\n",
    "            except Exception:\n",
    "                lang_conf = 0.0\n",
    "\n",
    "            combined_ml = ml['text']\n",
    "            combined_llm = llm['text']\n",
    "            content_hash = compute_content_hash(combined_ml)\n",
    "\n",
    "            to_upsert.append((post_id, lang, lang_conf, combined_raw, combined_ml, combined_llm, content_hash, PROC_VERSION))\n",
    "\n",
    "            last_pk = post_id\n",
    "            last_time = float(created_utc) if created_utc is not None else None\n",
    "\n",
    "        con.execute(\"BEGIN\")\n",
    "        upsert_posts_processed(con, to_upsert)\n",
    "        upsert_checkpoint(con, target, source, pk_col, time_col, last_pk, last_time)\n",
    "        con.commit()\n",
    "\n",
    "        return len(to_upsert), last_pk, last_time\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "\n",
    "def process_comments_batch(db_path: Optional[str] = None,\n",
    "                           batch_size: int = 200,\n",
    "                           to_lowercase_ml: bool = False) -> Tuple[int, Optional[str], Optional[float]]:\n",
    "    \"\"\"\n",
    "    Procesa reddit_comments -> _01_reddit_comments_preprocessed.\n",
    "    Devuelve: (n_insertados, last_pk, last_time)\n",
    "    \"\"\"\n",
    "    con = get_connection(db_path)\n",
    "    try:\n",
    "        target = \"_01_reddit_comments_preprocessed\"\n",
    "        source = \"reddit_comments\"\n",
    "        pk_col = \"comment_id\"\n",
    "        time_col = \"created_utc\"\n",
    "        select_cols = [\"comment_id\", \"post_id\", \"body\", \"created_utc\"]\n",
    "\n",
    "        ckpt = get_checkpoint(con, target)\n",
    "        rows = fetch_unprocessed_batch(con, source, target, pk_col, time_col, select_cols, batch_size, ckpt)\n",
    "\n",
    "        if not rows:\n",
    "            return 0, None, None\n",
    "\n",
    "        to_upsert = []\n",
    "        last_pk = None\n",
    "        last_time = None\n",
    "\n",
    "        for comment_id, post_id, body, created_utc in rows:\n",
    "            body_raw = _ensure_str(body)\n",
    "            ml = text_preprocessing_ml(body_raw, to_lowercase=to_lowercase_ml)\n",
    "            llm = text_preprocessing_llm(body_raw, to_lowercase=False)\n",
    "\n",
    "            lang = ml[\"lang\"]\n",
    "            try:\n",
    "                lang_conf = float(ml[\"lang_conf\"])\n",
    "            except Exception:\n",
    "                lang_conf = 0.0\n",
    "\n",
    "            body_ml = ml[\"text\"]\n",
    "            body_llm = llm[\"text\"]\n",
    "            content_hash = compute_content_hash(body_ml)\n",
    "\n",
    "            to_upsert.append((\n",
    "                comment_id, post_id, lang, lang_conf, body_raw, body_ml, body_llm, content_hash, PROC_VERSION\n",
    "            ))\n",
    "\n",
    "            last_pk = comment_id\n",
    "            last_time = float(created_utc) if created_utc is not None else None\n",
    "\n",
    "        con.execute(\"BEGIN\")\n",
    "        upsert_comments_processed(con, to_upsert)\n",
    "        upsert_checkpoint(con, target, source, pk_col, time_col, last_pk, last_time)\n",
    "        con.commit()\n",
    "\n",
    "        return len(to_upsert), last_pk, last_time\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "\n",
    "\n",
    "def process_news_batch(db_path: Optional[str] = None,\n",
    "                       batch_size: int = 200,\n",
    "                       to_lowercase_ml: bool = False) -> Tuple[int, Optional[str], Optional[float]]:\n",
    "    \"\"\"\n",
    "    Procesa news_articles -> _01_news_articles_preprocessed.\n",
    "    Devuelve: (n_insertados, last_pk, last_time)\n",
    "    \"\"\"\n",
    "\n",
    "    def _epoch_from_news_row(con, published_at, fetch_date):\n",
    "        # published_at y fetch_date pueden venir como ISO strings; convertimos a epoch\n",
    "        if isinstance(published_at, (int, float)):\n",
    "            pa = float(published_at)\n",
    "        else:\n",
    "            pa = con.execute(\"SELECT CAST(strftime('%s', ?) AS REAL);\", (published_at,)).fetchone()[0] if published_at else None\n",
    "\n",
    "        if pa is None:\n",
    "            fd = con.execute(\"SELECT CAST(strftime('%s', ?) AS REAL);\", (fetch_date,)).fetchone()[0] if fetch_date else None\n",
    "            return float(fd) if fd is not None else 0.0\n",
    "        return float(pa)\n",
    "\n",
    "    con = get_connection(db_path)\n",
    "    try:\n",
    "        target = \"_01_news_articles_preprocessed\"\n",
    "        source = \"news_articles\"\n",
    "        pk_col = \"url\"\n",
    "        time_col = \"published_at\"\n",
    "\n",
    "        # IMPORTANTE: incluye fetch_date para fallback en order_time y checkpoint\n",
    "        select_cols = [\"url\", \"title\", \"description\", \"content\", \"published_at\", \"fetch_date\"]\n",
    "\n",
    "        ckpt = get_checkpoint(con, target)\n",
    "        rows = fetch_unprocessed_batch(con, source, target, pk_col, time_col, select_cols, batch_size, ckpt)\n",
    "\n",
    "        if not rows:\n",
    "            return 0, None, None\n",
    "\n",
    "        to_upsert = []\n",
    "        last_pk = None\n",
    "        last_time = None\n",
    "\n",
    "        for url, title, description, content, published_at, fetch_date in rows:\n",
    "            # âœ… Usa SIEMPRE este last_time coherente con fetch_unprocessed_batch\n",
    "            last_time = _epoch_from_news_row(con, published_at, fetch_date)\n",
    "            last_pk = url\n",
    "\n",
    "            combined_raw = combine_news_text(title, description, content)\n",
    "            ml = text_preprocessing_ml(combined_raw, to_lowercase=to_lowercase_ml)\n",
    "            llm = text_preprocessing_llm(combined_raw, to_lowercase=False)\n",
    "\n",
    "            lang = ml[\"lang\"]\n",
    "            try:\n",
    "                lang_conf = float(ml[\"lang_conf\"])\n",
    "            except Exception:\n",
    "                lang_conf = 0.0\n",
    "\n",
    "            combined_ml = ml[\"text\"]\n",
    "            combined_llm = llm[\"text\"]\n",
    "            content_hash = compute_content_hash(combined_ml)\n",
    "\n",
    "            to_upsert.append((\n",
    "                url, lang, lang_conf, combined_raw, combined_ml, combined_llm, content_hash, PROC_VERSION\n",
    "            ))\n",
    "\n",
    "        con.execute(\"BEGIN\")\n",
    "        upsert_news_processed(con, to_upsert)\n",
    "        # âœ… Guarda el checkpoint del ÃšLTIMO registro del lote (ordenado por time, pk)\n",
    "        upsert_checkpoint(con, target, source, pk_col, time_col, last_pk, last_time)\n",
    "        con.commit()\n",
    "\n",
    "        return len(to_upsert), last_pk, last_time\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "            \n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead81559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all_tables(db_path: Optional[str] = None,\n",
    "                   sample_size_per_table: int = 200,\n",
    "                   batch_size: int = 50,\n",
    "                   to_lowercase_ml: bool = False):\n",
    "    '''\n",
    "    Migrates 'sample_size_per_table' rows per table in batches with 'batch_size' size\n",
    "    - If there are less rows pending, processes what there is\n",
    "    - Continues from the checkpoint\n",
    "    '''\n",
    "\n",
    "    db_path = db_path  or DB_PATH_DEFAULT\n",
    "    print(f'[SMOKE TEST] DB = {db_path}')\n",
    "    create_processed_schema(db_path)\n",
    "\n",
    "    processed = {\"posts\": 0, \"comments\": 0, \"news\": 0}\n",
    "\n",
    "    # POSTS\n",
    "    while processed['posts'] < sample_size_per_table:\n",
    "        n, last_pk, last_time = process_posts_batch(db_path, batch_size, to_lowercase_ml)\n",
    "        if n == 0:\n",
    "            break\n",
    "        processed[\"posts\"] += n\n",
    "        print(f' > posts +{n} (last_pk={last_pk})')\n",
    "\n",
    "    #COMMENTS\n",
    "    while processed['comments'] < sample_size_per_table:\n",
    "        n, last_pk, last_time = process_comments_batch(db_path, batch_size, to_lowercase_ml)\n",
    "        if n == 0:\n",
    "            break\n",
    "        processed[\"comments\"] += n\n",
    "        print(f' > comments +{n} (last_pk={last_pk})')\n",
    "\n",
    "    # NEWS\n",
    "\n",
    "    while processed['news'] < sample_size_per_table:\n",
    "        n, last_pk, last_time = process_news_batch(db_path, batch_size, to_lowercase_ml)\n",
    "        if n == 0:\n",
    "            break\n",
    "        processed[\"news\"] += n\n",
    "        print(f' > news +{n} (last_pk={last_pk})')\n",
    "\n",
    "    print('[DONE]', processed)\n",
    "\n",
    "    return processed\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "103d7582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_posts(con: sqlite3.Connection, limit: int = 10):\n",
    "    return con.execute(\"\"\"\n",
    "        SELECT post_id, subreddit, title, body, created_utc\n",
    "        FROM reddit_posts\n",
    "        ORDER BY created_utc DESC\n",
    "        LIMIT ?;\n",
    "    \"\"\", (limit,)).fetchall()\n",
    "\n",
    "def read_comments(con: sqlite3.Connection, limit: int = 10):\n",
    "    return con.execute(\"\"\"\n",
    "        SELECT comment_id, post_id, body, created_utc\n",
    "        FROM reddit_comments\n",
    "        ORDER BY created_utc DESC\n",
    "        LIMIT ?;\n",
    "    \"\"\", (limit,)).fetchall()\n",
    "\n",
    "def read_news(con: sqlite3.Connection, limit: int = 10):\n",
    "    return con.execute(\"\"\"\n",
    "        SELECT url, source_name, author, title, description, published_at\n",
    "        FROM news_articles\n",
    "        ORDER BY published_at DESC\n",
    "        LIMIT ?;\n",
    "    \"\"\", (limit,)).fetchall()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "investenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
