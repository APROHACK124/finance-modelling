{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583ec0b4",
   "metadata": {},
   "source": [
    "### 02_SQLite_IO_and_Batching\n",
    "\n",
    "Este bloque crea utilidades para:\n",
    "- Conectarse a SQLite con **WAL** y `synchronous=NORMAL`.\n",
    "- Crear tablas **_01_reddit_posts_preprocessed**, **_01_reddit_comments_preprocessed**, **_01_news_articles_preprocessed**.\n",
    "- Procesar texto en **lotes** con **checkpoint** (`_etl_checkpoints`) para reanudar.\n",
    "- Hacer un **smoke test** que migra N filas por tabla.\n",
    "\n",
    "**Notas de modelado**\n",
    "- Las tablas preprocesadas **no duplican** los metadatos crudos (p. ej. `score`, `url` de post), solo almacenan salidas de NLP + trazabilidad mínima.\n",
    "- `lang/lang_conf`: se detectan sobre el **texto combinado** (p. ej., `title + body`).\n",
    "- `content_hash`: hash estable del `combined_ml` para *dedup*/auditoría futura.\n",
    "- PK de preprocesadas = PK de crudas + `FOREIGN KEY ... ON DELETE CASCADE`, así mantienes integridad referencial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12193c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from python_scripts.utilities_text_processing import text_preprocessing, text_preprocessing_llm, text_preprocessing_ml\n",
    "\n",
    "# Default route\n",
    "DB_PATH_DEFAULT = os.path.expanduser(\n",
    "    \"~/Desktop/all_folders/Investings_project/app/data/stock_data.db\"\n",
    ")\n",
    "\n",
    "PROC_VERSION = \"01_preprocess_v0.1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d323516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'foreign_keys': 1, 'journal_mode': 'wal', 'synchronous': 1}\n"
     ]
    }
   ],
   "source": [
    "def get_connection(db_path: Optional[str] = None) -> sqlite3.Connection:\n",
    "    '''\n",
    "    Opens a connection to SQLite and appies PRAGMAs for efficiency\n",
    "    foreign_keys=ON (integrity)\n",
    "    journal_mode=WAL (better concurrency)\n",
    "    synchronous=NORMAL (decent efficiency with WAL)\n",
    "    '''\n",
    "    db_path = db_path or DB_PATH_DEFAULT\n",
    "    con = sqlite3.connect(db_path)\n",
    "    con.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "    mode = con.execute(\"PRAGMA journal_mode;\").fetchone()[0]\n",
    "    if mode.upper() != \"WAL\":\n",
    "        con.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "    con.execute(\"PRAGMA synchronous=NORMAL;\")\n",
    "    return con\n",
    "\n",
    "def check_pragmas(con: sqlite3.Connection) -> Dict[str, str]:\n",
    "    return {\n",
    "        'foreign_keys': con.execute(\"PRAGMA foreign_keys;\").fetchone()[0],\n",
    "        \"journal_mode\": con.execute(\"PRAGMA journal_mode;\").fetchone()[0],\n",
    "        \"synchronous\": con.execute(\"PRAGMA synchronous;\").fetchone()[0],\n",
    "    }\n",
    "\n",
    "con = get_connection()\n",
    "print(check_pragmas(con))\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb7999",
   "metadata": {},
   "source": [
    "# Table schemes + Indices + Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe13e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DDL_PREPROCESSED = {\n",
    "    \"_01_reddit_posts_preprocessed\": \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS _01_reddit_posts_preprocessed (\n",
    "        post_id TEXT PRIMARY KEY,\n",
    "        lang TEXT,\n",
    "        lang_conf REAL,\n",
    "        combined_raw TEXT,        -- title + body\n",
    "        combined_ml TEXT,\n",
    "        combined_llm TEXT,\n",
    "        content_hash TEXT,         -- hash combined_ml normalized\n",
    "        processed_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "        proc_version TEXT,\n",
    "        FOREIGN KEY (post_id) REFERENCES reddit_posts (post_id) ON DELETE CASCADE\n",
    "    );\n",
    "    \"\"\",\n",
    "\n",
    "    \"_01_reddit_comments_preprocessed\": \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS _01_reddit_comments_preprocessed (\n",
    "        comment_id TEXT PRIMARY KEY,\n",
    "        post_id TEXT NOT NULL,\n",
    "        lang TEXT,\n",
    "        lang_conf REAL,\n",
    "        body_raw TEXT, \n",
    "        body_ml TEXT,\n",
    "        body_llm TEXT,\n",
    "        content_hash TEXT,\n",
    "        processed_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "        proc_version TEXT,\n",
    "        FOREIGN KEY (comment_id) REFERENCES reddit_comments (commend_id) ON DELETE CASCADE\n",
    "        FOREIGN KEY (post_id) REFERENCES reddit_posts (post_id) ON DELETE CASCADE\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"_01_news_articles_preprocessed\": \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS _01_news_articles_preprocessed (\n",
    "            url TEXT PRIMARY KEY,\n",
    "            lang TEXT,\n",
    "            lang_conf REAL,\n",
    "            combined_raw TEXT,     -- title + description + content\n",
    "            combined_ml TEXT,\n",
    "            combined_llm TEXT,\n",
    "            content_hash TEXT,\n",
    "            processed_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "            proc_version TEXT,\n",
    "            FOREIGN KEY (url) REFERENCES new_articles (url) ON DELETE CASCADE\n",
    "        );\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "DDL_INDEXES = {\n",
    "    \"_01_reddit_posts_preprocessed\": [\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_rpp_hash ON _01_reddit_posts_preprocessed (content_hash);\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_rpp_proc_at ON _01_reddit_posts_preprocessed (processed_at);\",\n",
    "    ],\n",
    "    \"_01_reddit_comments_preprocessed\": [\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_rcp_hash ON _01_reddit_comments_preprocessed (content_hash);\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_rcp_post ON _01_reddit_comments_preprocessed (post_id);\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_rcp_proc_at ON _01_reddit_comments_preprocessed (processed_at);\",\n",
    "    ],\n",
    "    \"_01_news_articles_preprocessed\": [\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_nap_hash ON _01_news_articles_preprocessed (content_hash);\",\n",
    "        \"CREATE INDEX IF NOT EXISTS idx_nap_proc_at ON _01_news_articles_preprocessed (processed_at);\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "DDL_CHECKPOINTS = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS _etl_checkpoints (\n",
    "    target_table TEXT PRIMARY KEY,    -- p.ej. '_01_reddit_posts_preprocessed'\n",
    "    source_table TEXT NOT NULL,       -- p.ej. 'reddit_posts'\n",
    "    pk_col       TEXT NOT NULL,       -- p.ej. 'post_id'\n",
    "    time_col     TEXT NOT NULL,       -- p.ej. 'created_utc' | 'published_at'\n",
    "    last_pk      TEXT,                -- último PK procesado\n",
    "    last_time    REAL,                -- último timestamp procesado (segundos UNIX)\n",
    "    updated_at   DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Vistas de conveniencia (joins listos para usar)\n",
    "DDL_VIEWS = {\n",
    "    \"v_reddit_posts_enriched\": \"\"\"\n",
    "    CREATE VIEW IF NOT EXISTS v_reddit_posts_enriched AS\n",
    "    SELECT p.*, pp.lang, pp.lang_conf, pp.combined_ml, pp.combined_llm, pp.content_hash, pp.processed_at\n",
    "    FROM reddit_posts p\n",
    "    LEFT JOIN _01_reddit_posts_preprocessed pp ON pp.post_id = p.post_id;\n",
    "    \"\"\",\n",
    "    \"v_reddit_comments_enriched\": \"\"\"\n",
    "    CREATE VIEW IF NOT EXISTS v_reddit_comments_enriched AS\n",
    "    SELECT c.*, cp.lang, cp.lang_conf, cp.body_ml, cp.body_llm, cp.content_hash, cp.processed_at\n",
    "    FROM reddit_comments c\n",
    "    LEFT JOIN _01_reddit_comments_preprocessed cp ON cp.comment_id = c.comment_id;\n",
    "    \"\"\",\n",
    "    \"v_news_articles_enriched\": \"\"\"\n",
    "    CREATE VIEW IF NOT EXISTS v_news_articles_enriched AS\n",
    "    SELECT n.*, np.lang, np.lang_conf, np.combined_ml, np.combined_llm, np.content_hash, np.processed_at\n",
    "    FROM news_articles n\n",
    "    LEFT JOIN _01_news_articles_preprocessed np ON np.url = n.url;\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a706f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_processed_schema(db_path: Optional[str] = None):\n",
    "    con = get_connection(db_path)\n",
    "    try:\n",
    "        cur = con.cursor()\n",
    "        for ddl in DDL_PREPROCESSED.values():\n",
    "            cur.execute(ddl)\n",
    "        for tbl, idxs in DDL_INDEXES.items():\n",
    "            for idx in idxs:\n",
    "                cur.execute(idx)\n",
    "        cur.execute(DDL_CHECKPOINTS)\n",
    "        for ddl in DDL_VIEWS.values():\n",
    "            cur.execute(ddl)\n",
    "        # new\n",
    "        cur.execute('''\n",
    "            CREATE INDEX IF NOT EXISTS idx_stock_indicators_sym_tf_ts\n",
    "            ON stock_indicators(symbol, timeframe, timestamp);\n",
    "\n",
    "        ''')\n",
    "        con.commit()\n",
    "\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "create_processed_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0722583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_str(x) -> str:\n",
    "    return \"\" if x is None else str(x)\n",
    "\n",
    "def compute_content_hash(text: str) -> str:\n",
    "    norm = (_ensure_str(text)).strip()\n",
    "    h = hashlib.blake2b(norm.encode('utf-8'), digest_size = 20)\n",
    "    return h.hexdigest()\n",
    "def combine_post_text(title: Optional[str], body: Optional[str]) -> str:\n",
    "    parts = [p.strip() for p in [title, body] if p and p.strip()]\n",
    "    return \" - \".join(parts) if parts else \"\"\n",
    "\n",
    "def combine_news_text(title: Optional[str], desc: Optional[str], content: Optional[str]) -> str:\n",
    "    parts = [p.strip() for p in [title, desc, content] if p and p.strip()]\n",
    "    return \" - \".join(parts) if parts else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b72b31bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_checkpoint(con: sqlite3.Connection,\n",
    "                      target_table: str,\n",
    "                      source_table: str,\n",
    "                      pk_col: str,\n",
    "                      time_col: str,\n",
    "                      last_pk: Optional[str],\n",
    "                      last_time: Optional[float]\n",
    "                      ) -> None:\n",
    "    con.execute(\"\"\"\n",
    "        INSERT INTO _etl_checkpoints(target_table, source_table, pk_col, time_col, last_pk, last_time, updated_at)\n",
    "        VALUES (?, ?, ?, ?, ? , ?, CURRENT_TIMESTAMP)\n",
    "        ON CONFLICT(target_table) DO UPDATE SET\n",
    "            last_pk = excluded.last_pk,\n",
    "            last_time = excluded.last_time,\n",
    "            updated_at = CURRENT_TIMESTAMP;\n",
    "        \"\"\", (target_table, source_table, pk_col, time_col, last_pk, last_time))\n",
    "    \n",
    "def get_checkpoint(con: sqlite3.Connection, target_table: str) -> Optional[Tuple[str, str, str, str, Optional[str], Optional[float]]]:\n",
    "    row = con.execute(\"\"\"\n",
    "                      SELECT target_table, source_table, pk_col, time_col, last_pk, last_time\n",
    "                      FROM _etl_checkpoints WHERE target_table = ?;\n",
    "                      \"\"\", (target_table)).fetchone()\n",
    "    return row\n",
    "\n",
    "def _order_time_sql(source_table: str, time_col: str) -> str:\n",
    "    # Returns the SQL expression that produces order_time in UNIX seconds\n",
    "\n",
    "    if source_table in (\"reddit_posts\", \"reddit_comments\") and time_col == \"created_utc\":\n",
    "        return f\"{source_table}.created_utc\"\n",
    "    if source_table == \"news_articles\" and time_col == \"published_at\":\n",
    "        return f\"CAST(strtime('%s', {source_table}.published_at) AS REAL)\"\n",
    "    return f\"CAST(strftime('%s', {source_table}.{time_col}) AS REAL)\"\n",
    "\n",
    "def fetch_unprocessed_batch(con: sqlite3.Connection,\n",
    "                            source_table: str,\n",
    "                            processed_table: str,\n",
    "                            pk_col: str,\n",
    "                            time_col: str,\n",
    "                            select_cols: List[str],\n",
    "                            batch_size: int,\n",
    "                            checkpoint: Optional[Tuple[str, str, str, str, Optional[str], Optional[float]]] = None):\n",
    "    '''\n",
    "    Obtains some crude rows that arent in the preprocessed table\n",
    "    Respects checkpoint: (last_time, last_pk) for sorting and continuing\n",
    "    '''\n",
    "\n",
    "    order_time = _order_time_sql(source_table, time_col)\n",
    "    last_pk = None\n",
    "    last_time = None\n",
    "    if checkpoint:\n",
    "        _, src_tbl, _, _, last_pk, last_time = checkpoint\n",
    "    \n",
    "    # Criteria: \"not exists\" + checkpoint per (time, pk) for an stable order\n",
    "    # if last_time/last_pk are NULL, the condition is ignored (first time)\n",
    "\n",
    "    where_checkpoint = f\"\"\"\n",
    "    AND (\n",
    "        (? IS NULL AND ? IS NULL)\n",
    "        OR\n",
    "        ({order_time} > ?)\n",
    "        OR\n",
    "        ({order_time} = ? AND {source_table}.{pk_col} > ?)\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    SELECT {\", \".join([f\"{source_table}.{c}\" for c in select_cols])}\n",
    "    FROM {source_table}\n",
    "    LEFT JOIN {processed_table} p ON p.{pk_col} = {source_table}.{pk_col}\n",
    "    WHERE p.{pk_col} IS NULL\n",
    "    {where_checkpoint}\n",
    "    ORDER BY {order_time} ASC, {source_table}.{pk_col} ASC\n",
    "    LIMIT ?;\n",
    "    \"\"\"\n",
    "    params = (last_time, last_pk, last_time, last_time, last_pk, batch_size)\n",
    "    cur = con.execute(sql, params)\n",
    "    rows = cur.fetchall()\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f43d4",
   "metadata": {},
   "source": [
    "# Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7d14bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_posts_processed(con: sqlite3.Connection, rows: List[Tuple]):\n",
    "    \"\"\"\n",
    "    Inserta/actualiza filas en _01_reddit_posts_preprocessed.\n",
    "    'rows' es una lista de tuplas con:\n",
    "      (post_id, lang, lang_conf, combined_raw, combined_ml, combined_llm, content_hash, PROC_VERSION)\n",
    "    \"\"\"\n",
    "    con.executemany(\"\"\"\n",
    "        INSERT INTO _01_reddit_posts_preprocessed(\n",
    "            post_id, lang, lang_conf, combined_raw, combined_ml, combined_llm, content_hash, proc_version\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ON CONFLICT(post_id) DO UPDATE SET\n",
    "            lang = excluded.lang,\n",
    "            lang_conf = excluded.lang_conf,\n",
    "            combined_raw = excluded.combined_raw,\n",
    "            combined_ml = excluded.combined_ml,\n",
    "            combined_llm = excluded.combined_llm,\n",
    "            content_hash = excluded.content_hash,\n",
    "            processed_at = CURRENT_TIMESTAMP,\n",
    "            proc_version = excluded.proc_version;\n",
    "    \"\"\", rows)\n",
    "\n",
    "def upsert_comments_processed(con: sqlite3.Connection, rows: List[Tuple]):\n",
    "    \"\"\"\n",
    "    Inserta/actualiza filas en _01_reddit_comments_preprocessed.\n",
    "    Tuplas:\n",
    "      (comment_id, post_id, lang, lang_conf, body_raw, body_ml, body_llm, content_hash, PROC_VERSION)\n",
    "    \"\"\"\n",
    "    con.executemany(\"\"\"\n",
    "        INSERT INTO _01_reddit_comments_preprocessed(\n",
    "            comment_id, post_id, lang, lang_conf, body_raw, body_ml, body_llm, content_hash, proc_version\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ON CONFLICT(comment_id) DO UPDATE SET\n",
    "            post_id = excluded.post_id,\n",
    "            lang = excluded.lang,\n",
    "            lang_conf = excluded.lang_conf,\n",
    "            body_raw = excluded.body_raw,\n",
    "            body_ml = excluded.body_ml,\n",
    "            body_llm = excluded.body_llm,\n",
    "            content_hash = excluded.content_hash,\n",
    "            processed_at = CURRENT_TIMESTAMP,\n",
    "            proc_version = excluded.proc_version;\n",
    "    \"\"\", rows)\n",
    "\n",
    "def upsert_news_processed(con: sqlite3.Connection, rows: List[Tuple]):\n",
    "    \"\"\"\n",
    "    Inserta/actualiza filas en _01_news_articles_preprocessed.\n",
    "    Tuplas:\n",
    "      (url, lang, lang_conf, combined_raw, combined_ml, combined_llm, content_hash, PROC_VERSION)\n",
    "    \"\"\"\n",
    "    con.executemany(\"\"\"\n",
    "        INSERT INTO _01_news_articles_preprocessed(\n",
    "            url, lang, lang_conf, combined_raw, combined_ml, combined_llm, content_hash, proc_version\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ON CONFLICT(url) DO UPDATE SET\n",
    "            lang = excluded.lang,\n",
    "            lang_conf = excluded.lang_conf,\n",
    "            combined_raw = excluded.combined_raw,\n",
    "            combined_ml = excluded.combined_ml,\n",
    "            combined_llm = excluded.combined_llm,\n",
    "            content_hash = excluded.content_hash,\n",
    "            processed_at = CURRENT_TIMESTAMP,\n",
    "            proc_version = excluded.proc_version;\n",
    "    \"\"\", rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d7a00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from click import Option\n",
    "\n",
    "\n",
    "def process_posts_batch(db_path: Optional[str] = None,\n",
    "                        batch_size: int = 100,\n",
    "                        to_lowercase_ml: bool = False) -> Tuple[int, Optional[str], Optional[float]]:\n",
    "    # Returns (n_inserted, last_pk, last_time)\n",
    "    con = get_connection(db_path)\n",
    "    try:\n",
    "        target = \"_01_reddit_posts_preprocessed\"\n",
    "        source = \"reddit_posts\"\n",
    "        pk_col = \"post_id\"\n",
    "        time_col = \"created_utc\"\n",
    "        select_cols = [\"post_id\", \"title\", \"body\", \"created_utc\"]\n",
    "\n",
    "        ckpt = get_checkpoint(con, target)\n",
    "\n",
    "        rows = fetch_unprocessed_batch(con, source, target, pk_col, time_col, select_cols, batch_size, ckpt)\n",
    "        if not rows:\n",
    "            return 0, None, None\n",
    "        \n",
    "        to_upsert = []\n",
    "        lsat_pk = None\n",
    "        last_time = None\n",
    "\n",
    "        for post_id, title, body, created_utc in rows:\n",
    "            combined_raw = combine_post_text(title, body)\n",
    "            ml = text_preprocessing_ml(combined_raw, to_lowercase_ml)\n",
    "            llm = text_preprocessing_llm(combined_raw, to_lowercase=False)\n",
    "\n",
    "            lang = ml[\"lang\"]\n",
    "            try:\n",
    "                lang_conf = float(ml['lang_conf'])\n",
    "            except Exception:\n",
    "                lang_conf = 0.0\n",
    "\n",
    "            combined_ml = ml['text']\n",
    "            combined_llm = llm['text']\n",
    "            content_hash = compute_content_hash(combined_ml)\n",
    "\n",
    "            to_upsert.append((post_id, lang, lang_conf, combined_raw, combined_ml, combined_llm, content_hash, PROC_VERSION))\n",
    "\n",
    "            last_pk = post_id\n",
    "            last_time = float(created_utc) if created_utc is not None else None\n",
    "\n",
    "        con.execute(\"BEGIN\")\n",
    "        upsert_posts_processed(con, to_upsert)\n",
    "        upsert_checkpoint(con, target, source, pk_col, time_col, last_pk, last_time)\n",
    "        con.commit()\n",
    "\n",
    "        return len(to_upsert), last_pk, last_time\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "\n",
    "def process_comments_batch(db_path: Optional[str] = None,\n",
    "                           batch_size: int = 200,\n",
    "                           to_lowercase_ml: bool = False) -> Tuple[int, Optional[str], Optional[float]]:\n",
    "    \"\"\"\n",
    "    Procesa reddit_comments -> _01_reddit_comments_preprocessed.\n",
    "    Devuelve: (n_insertados, last_pk, last_time)\n",
    "    \"\"\"\n",
    "    con = get_connection(db_path)\n",
    "    try:\n",
    "        target = \"_01_reddit_comments_preprocessed\"\n",
    "        source = \"reddit_comments\"\n",
    "        pk_col = \"comment_id\"\n",
    "        time_col = \"created_utc\"\n",
    "        select_cols = [\"comment_id\", \"post_id\", \"body\", \"created_utc\"]\n",
    "\n",
    "        ckpt = get_checkpoint(con, target)\n",
    "        rows = fetch_unprocessed_batch(con, source, target, pk_col, time_col, select_cols, batch_size, ckpt)\n",
    "\n",
    "        if not rows:\n",
    "            return 0, None, None\n",
    "\n",
    "        to_upsert = []\n",
    "        last_pk = None\n",
    "        last_time = None\n",
    "\n",
    "        for comment_id, post_id, body, created_utc in rows:\n",
    "            body_raw = _ensure_str(body)\n",
    "            ml = text_preprocessing_ml(body_raw, to_lowercase=to_lowercase_ml)\n",
    "            llm = text_preprocessing_llm(body_raw, to_lowercase=False)\n",
    "\n",
    "            lang = ml[\"lang\"]\n",
    "            try:\n",
    "                lang_conf = float(ml[\"lang_conf\"])\n",
    "            except Exception:\n",
    "                lang_conf = 0.0\n",
    "\n",
    "            body_ml = ml[\"text\"]\n",
    "            body_llm = llm[\"text\"]\n",
    "            content_hash = compute_content_hash(body_ml)\n",
    "\n",
    "            to_upsert.append((\n",
    "                comment_id, post_id, lang, lang_conf, body_raw, body_ml, body_llm, content_hash, PROC_VERSION\n",
    "            ))\n",
    "\n",
    "            last_pk = comment_id\n",
    "            last_time = float(created_utc) if created_utc is not None else None\n",
    "\n",
    "        con.execute(\"BEGIN\")\n",
    "        upsert_comments_processed(con, to_upsert)\n",
    "        upsert_checkpoint(con, target, source, pk_col, time_col, last_pk, last_time)\n",
    "        con.commit()\n",
    "\n",
    "        return len(to_upsert), last_pk, last_time\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "\n",
    "\n",
    "def process_news_batch(db_path: Optional[str] = None,\n",
    "                       batch_size: int = 200,\n",
    "                       to_lowercase_ml: bool = False) -> Tuple[int, Optional[str], Optional[float]]:\n",
    "    \"\"\"\n",
    "    Procesa news_articles -> _01_news_articles_preprocessed.\n",
    "    Devuelve: (n_insertados, last_pk, last_time)\n",
    "    \"\"\"\n",
    "    con = get_connection(db_path)\n",
    "    try:\n",
    "        target = \"_01_news_articles_preprocessed\"\n",
    "        source = \"news_articles\"\n",
    "        pk_col = \"url\"\n",
    "        time_col = \"published_at\"\n",
    "        select_cols = [\"url\", \"title\", \"description\", \"content\", \"published_at\"]\n",
    "\n",
    "        ckpt = get_checkpoint(con, target)\n",
    "        rows = fetch_unprocessed_batch(con, source, target, pk_col, time_col, select_cols, batch_size, ckpt)\n",
    "\n",
    "        if not rows:\n",
    "            return 0, None, None\n",
    "\n",
    "        to_upsert = []\n",
    "        last_pk = None\n",
    "        last_time = None\n",
    "\n",
    "        for url, title, description, content, published_at in rows:\n",
    "            combined_raw = combine_news_text(title, description, content)\n",
    "            ml = text_preprocessing_ml(combined_raw, to_lowercase=to_lowercase_ml)\n",
    "            llm = text_preprocessing_llm(combined_raw, to_lowercase=False)\n",
    "\n",
    "            lang = ml[\"lang\"]\n",
    "            try:\n",
    "                lang_conf = float(ml[\"lang_conf\"])\n",
    "            except Exception:\n",
    "                lang_conf = 0.0\n",
    "\n",
    "            combined_ml = ml[\"text\"]\n",
    "            combined_llm = llm[\"text\"]\n",
    "            content_hash = compute_content_hash(combined_ml)\n",
    "\n",
    "            to_upsert.append((\n",
    "                url, lang, lang_conf, combined_raw, combined_ml, combined_llm, content_hash, PROC_VERSION\n",
    "            ))\n",
    "\n",
    "            # published_at a UNIX secs (puede venir como string ISO)\n",
    "            try:\n",
    "                # Con SQL ya ordenamos con strftime('%s'), aquí solo para checkpoint\n",
    "                # Intento convertir manualmente en caso necesario:\n",
    "                if isinstance(published_at, (int, float)):\n",
    "                    last_time = float(published_at)\n",
    "                else:\n",
    "                    # Intentar parseo flexible\n",
    "                    # Nota: si falla, dejamos last_time=None para no romper\n",
    "                    last_time = None\n",
    "            except Exception:\n",
    "                last_time = None\n",
    "\n",
    "            last_pk = url\n",
    "\n",
    "        con.execute(\"BEGIN\")\n",
    "        upsert_news_processed(con, to_upsert)\n",
    "        upsert_checkpoint(con, target, source, pk_col, time_col, last_pk, last_time)\n",
    "        con.commit()\n",
    "\n",
    "        return len(to_upsert), last_pk, last_time\n",
    "    finally:\n",
    "        con.close()\n",
    "            \n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ead81559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_smoke_test(db_path: Optional[str] = None,\n",
    "                   sample_size_per_table: int = 200,\n",
    "                   batch_size: int = 50,\n",
    "                   to_lowercase_ml: bool = False):\n",
    "    '''\n",
    "    Migrates 'sample_size_per_table' rows per table in batches with 'batch_size' size\n",
    "    - If there are less rows pending, processes what there is\n",
    "    - Continues from the checkpoint\n",
    "    '''\n",
    "\n",
    "    db_path = db_path  or DB_PATH_DEFAULT\n",
    "    print(f'[SMOKE TEST] DB = {db_path}')\n",
    "    create_processed_schema(db_path)\n",
    "\n",
    "    processed = {\"posts\": 0, \"comments\": 0, \"news\": 0}\n",
    "\n",
    "    # POSTS\n",
    "    while processed['posts'] < sample_size_per_table:\n",
    "        n, last_pk, last_time = process_posts_batch(db_path, batch_size, to_lowercase_ml)\n",
    "        if n == 0:\n",
    "            break\n",
    "        processed[\"posts\"] += n\n",
    "        print(f' > posts +{n} (last_pk={last_pk})')\n",
    "\n",
    "    #COMMENTS\n",
    "    while processed['comments'] < sample_size_per_table:\n",
    "        n, last_pk, last_time = process_comments_batch(db_path, batch_size, to_lowercase_ml)\n",
    "        if n == 0:\n",
    "            break\n",
    "        processed[\"comments\"] += n\n",
    "        print(f' > comments +{n} (last_pk={last_pk})')\n",
    "\n",
    "    # NEWS\n",
    "\n",
    "    while processed['news'] < sample_size_per_table:\n",
    "        n, last_pk, last_time = process_news_batch(db_path, batch_size, to_lowercase_ml)\n",
    "        if n == 0:\n",
    "            break\n",
    "        processed[\"news\"] += n\n",
    "        print(f' > news +{n} (last_pk={last_pk})')\n",
    "\n",
    "    print('[DONE]', processed)\n",
    "\n",
    "    return processed\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf2bfd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SMOKE TEST] DB = /home/aprohack/Desktop/all_folders/Investings_project/app/data/stock_data.db\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "Incorrect number of bindings supplied. The current statement uses 1, and there are 29 supplied.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mProgrammingError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_smoke_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mrun_smoke_test\u001b[39m\u001b[34m(db_path, sample_size_per_table, batch_size, to_lowercase_ml)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# POSTS\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m processed[\u001b[33m'\u001b[39m\u001b[33mposts\u001b[39m\u001b[33m'\u001b[39m] < sample_size_per_table:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     n, last_pk, last_time = \u001b[43mprocess_posts_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_lowercase_ml\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n == \u001b[32m0\u001b[39m:\n\u001b[32m     21\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mprocess_posts_batch\u001b[39m\u001b[34m(db_path, batch_size, to_lowercase_ml)\u001b[39m\n\u001b[32m     13\u001b[39m time_col = \u001b[33m\"\u001b[39m\u001b[33mcreated_utc\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m select_cols = [\u001b[33m\"\u001b[39m\u001b[33mpost_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcreated_utc\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m ckpt = \u001b[43mget_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m rows = fetch_unprocessed_batch(con, source, target, pk_col, time_col, select_cols, batch_size, ckpt)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rows:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mget_checkpoint\u001b[39m\u001b[34m(con, target_table)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_checkpoint\u001b[39m(con: sqlite3.Connection, target_table: \u001b[38;5;28mstr\u001b[39m) -> Optional[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m], Optional[\u001b[38;5;28mfloat\u001b[39m]]]:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     row = \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[33;43m                      SELECT target_table, source_table, pk_col, time_col, last_pk, last_time\u001b[39;49m\n\u001b[32m     21\u001b[39m \u001b[33;43m                      FROM _etl_checkpoints WHERE target_table = ?;\u001b[39;49m\n\u001b[32m     22\u001b[39m \u001b[33;43m                      \u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_table\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.fetchone()\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "\u001b[31mProgrammingError\u001b[39m: Incorrect number of bindings supplied. The current statement uses 1, and there are 29 supplied."
     ]
    }
   ],
   "source": [
    "run_smoke_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d7582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1olrogq', 'news', \"80% of NYC-area air traffic controllers absent amid 'surge' in callouts: FAA\", '', 1762013344.0), ('1olr5tj', 'wallstreetbets', 'Does This Look Like A Man Worried About The Bubble', '', 1762012097.0), ('1olickz', 'news', 'Cruise cancelled following death of woman left behind on island', '', 1761984476.0)]\n",
      "[('nmkh9ox', '1ojj0wp', 'Hit?  You know they are going IPO in two years right?  They are buying in early and cheap.', 1762019748.0), ('nmjygn7', '1olrogq', 'All of the nations air traffic controllers should refuse to show up. This shutdown would be solved within hours of a complete aviation freeze.\\n\\nEdit: for those saying it is technically illegal for federal employees to go on strike. The government deems these federal jobs as too critical to be disrupted, yet they don’t deem food availability to some 42 million people on SNAP as critical. (We will see what becomes of the ruling this morning)… prosecuting all federal air traffic controllers would be a logistical nightmare for the United States government. The law is a paper dragon when that many people step off the job.', 1762013998.0), ('nmjy4vg', '1olrogq', 'At least we can still fund Nuke testing and illegal ICE raids', 1762013893.0)]\n",
      "[('https://www.barchart.com/story/news/35832951/a-135-billion-reason-to-buy-microsoft-stock-now', 'Barchart.com', 'Nauman Khan', 'A $135 Billion Reason to Buy Microsoft Stock Now', 'Microsoft’s $135B OpenAI stake deepens its AI moat, giving investors cloud scale, IP control and a fresh reason to buy the dip.', '2025-10-31T17:49:10Z'), ('https://biztoc.com/x/9e4e2a840dcde591', 'Biztoc.com', 'barchart.com', 'Amazon Stock Popped on Earnings. Options Data Tells Us AMZN Could Be Headed Here Next', '', '2025-10-31T17:36:54Z'), ('https://www.barchart.com/story/news/35832637/amazon-stock-popped-on-earnings-options-data-tells-us-amzn-could-be-headed-here-next', 'Barchart.com', 'Wajeeh Khan', 'Amazon Stock Popped on Earnings. Options Data Tells Us AMZN Could Be Headed Here Next.', 'Amazon stock soars as AWS posts its strongest quarterly growth since 2022. Options data suggests AMZN shares will push higher through the remainder of 2025.', '2025-10-31T17:28:04Z')]\n"
     ]
    }
   ],
   "source": [
    "def read_posts(con: sqlite3.Connection, limit: int = 10):\n",
    "    return con.execute(\"\"\"\n",
    "        SELECT post_id, subreddit, title, body, created_utc\n",
    "        FROM reddit_posts\n",
    "        ORDER BY created_utc DESC\n",
    "        LIMIT ?;\n",
    "    \"\"\", (limit,)).fetchall()\n",
    "\n",
    "def read_comments(con: sqlite3.Connection, limit: int = 10):\n",
    "    return con.execute(\"\"\"\n",
    "        SELECT comment_id, post_id, body, created_utc\n",
    "        FROM reddit_comments\n",
    "        ORDER BY created_utc DESC\n",
    "        LIMIT ?;\n",
    "    \"\"\", (limit,)).fetchall()\n",
    "\n",
    "def read_news(con: sqlite3.Connection, limit: int = 10):\n",
    "    return con.execute(\"\"\"\n",
    "        SELECT url, source_name, author, title, description, published_at\n",
    "        FROM news_articles\n",
    "        ORDER BY published_at DESC\n",
    "        LIMIT ?;\n",
    "    \"\"\", (limit,)).fetchall()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "investenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
