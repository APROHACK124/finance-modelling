{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56a61208",
   "metadata": {},
   "source": [
    "# imports + helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b59a232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DB_PATH_DEFAULT] /home/aprohack/Desktop/all_folders/Investings_project/app/data/stock_data.db\n",
      "{'foreign_keys': 1, 'journal_mode': 'wal', 'synchronous': 1}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "from machine_learning.data_collectors import (\n",
    "    build_ml_dataframe,\n",
    "    build_supervised_dataset,\n",
    "    time_split_masks,\n",
    "    purged_ts_cv_splits,\n",
    "    TARGET_HORIZONS,\n",
    "    TARGET_LOOKBACKS,\n",
    ")\n",
    "from machine_learning.artifacts import load_mlp_artifact, predict_artifact_to_compare\n",
    "from machine_learning.evaluators import calculate_deadzone\n",
    "from machine_learning.artifacts import discover_artifacts_by_horizon\n",
    "from machine_learning.evaluators import eval_regression, eval_regression_extended\n",
    "from database_tier1 import TARGET_STOCKS\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# Tus funciones existentes (ya están en tu proyecto)\n",
    "# build_ml_dataframe\n",
    "# build_supervised_dataset\n",
    "# eval_regression_extended\n",
    "# predict_artifact_to_compare\n",
    "\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _json_default(obj):\n",
    "    \"\"\"Para poder json.dump con numpy/pandas timestamps.\"\"\"\n",
    "    if isinstance(obj, (np.integer,)):\n",
    "        return int(obj)\n",
    "    if isinstance(obj, (np.floating,)):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, (np.ndarray,)):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, (pd.Timestamp,)):\n",
    "        return obj.isoformat()\n",
    "    if hasattr(obj, \"item\"):\n",
    "        try:\n",
    "            return obj.item()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return str(obj)\n",
    "\n",
    "def _safe_write_json(path: Path, payload: dict):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False, default=_json_default)\n",
    "\n",
    "def _save_df(path: Path, df: pd.DataFrame):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # Prefer parquet si está disponible; sino CSV\n",
    "    try:\n",
    "        df.to_parquet(path, index=False)\n",
    "    except Exception:\n",
    "        df.to_csv(path.with_suffix(\".csv\"), index=False)\n",
    "\n",
    "def save_sklearn_pipeline_artifact(\n",
    "    run_dir: str,\n",
    "    pipeline,\n",
    "    *,\n",
    "    config: dict,\n",
    "    metrics: dict,\n",
    "    feature_names: list[str],\n",
    "    pred_df_val: pd.DataFrame | None = None,\n",
    "    pred_df_test: pd.DataFrame | None = None,\n",
    "    extra: dict | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Guarda un artefacto estilo 'sklearn' compatible con tu auto-loader:\n",
    "      - pipeline.joblib\n",
    "      - feature_names.json\n",
    "      - config.json\n",
    "      - metrics.json\n",
    "      - pred_df_val.parquet (opcional)\n",
    "      - pred_df_test.parquet (opcional)\n",
    "      - extra.json (opcional)\n",
    "    \"\"\"\n",
    "    p = Path(run_dir)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    joblib.dump(pipeline, p / \"pipeline.joblib\")\n",
    "\n",
    "    _safe_write_json(p / \"feature_names.json\", list(feature_names))\n",
    "    _safe_write_json(p / \"config.json\", config)\n",
    "    _safe_write_json(p / \"metrics.json\", metrics)\n",
    "\n",
    "    if pred_df_val is not None:\n",
    "        _save_df(p / \"pred_df_val.parquet\", pred_df_val)\n",
    "\n",
    "    if pred_df_test is not None:\n",
    "        _save_df(p / \"pred_df_test.parquet\", pred_df_test)\n",
    "\n",
    "    if extra is not None:\n",
    "        _safe_write_json(p / \"extra.json\", extra)\n",
    "\n",
    "    return str(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39943bf5",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7bd458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lb=252, h=[60]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Config general del experimento\n",
    "# -----------------------\n",
    "from python_scripts.LLM_analysis.preprocess_store_database import get_connection\n",
    "\n",
    "conn = get_connection()\n",
    "\n",
    "timeframe = \"1Day\"\n",
    "symbols = TARGET_STOCKS\n",
    "\n",
    "start = None\n",
    "end = None\n",
    "\n",
    "include_indicators = True\n",
    "indicator_names = []\n",
    "indicator_names = ['RSI_14', 'BBB_20_2.0', 'BBP_20_2.0', 'ATRr_14']\n",
    "\n",
    "include_economic_indicators = False\n",
    "econ_indicator_names = []\n",
    "# econ_indicator_names = ['CPI', 'UNEMPLOYMENT']\n",
    "\n",
    "include_fmp = False\n",
    "fmp_feature_names = []\n",
    "keep_fmp_asof_date = False\n",
    "\n",
    "# -----------------------\n",
    "# ELIGE LOOKBACK AQUÍ\n",
    "# -----------------------\n",
    "lookback = TARGET_LOOKBACKS[-1]  # <-- cámbialo\n",
    "\n",
    "# 3 horizontes baseline (puedes editar)\n",
    "horizons = [60]\n",
    "\n",
    "# Lags control (opcional):\n",
    "# - Si lags_by_feature=None => TODOS los base_feature_cols usan 0..lookback-1 (puede explotar columnas si lookback grande).\n",
    "# - Si quieres algo más manejable para XGB, ejemplo:\n",
    "#     - OHLCV con lookback completo\n",
    "#     - indicadores/econ/fmp solo lag0\n",
    "lags_by_feature = {\n",
    "    \"close\": lookback,     \n",
    "    \"volume\": lookback,\n",
    "}\n",
    "default_lags = 0\n",
    "\n",
    "# Ejemplo recomendado si tus indicadores/econ/fmp son muchos:\n",
    "# lags_by_feature = {\"open\": lookback, \"high\": lookback, \"low\": lookback, \"close\": lookback, \"volume\": lookback}\n",
    "# default_lags = 1  # resto solo lag0\n",
    "\n",
    "print(f\"lb={lookback}, h={horizons}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb87e37",
   "metadata": {},
   "source": [
    "# Dataset construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a12ef10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape: (204334, 14)\n",
      "n base_feature_cols: 11\n",
      "example cols: ['open', 'high', 'low', 'close', 'volume', 'trade_count', 'vwap', 'RSI_14', 'BBB_20_2.0', 'BBP_20_2.0', 'ATRr_14']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# Construir el dataframe ML\n",
    "# -----------------------\n",
    "df = build_ml_dataframe(\n",
    "    conn=conn,\n",
    "    symbols=symbols,\n",
    "    timeframe=timeframe,\n",
    "    start=start,\n",
    "    end=end,\n",
    "    include_indicators=include_indicators,\n",
    "    indicator_names=indicator_names,\n",
    "    include_econ=include_economic_indicators,\n",
    "    econ_indicator_names=econ_indicator_names,\n",
    "    include_fmp=include_fmp,\n",
    "    fmp_feature_names=fmp_feature_names,\n",
    "    fmp_prefix=\"fmp\",                 # consistente con tu MLP config\n",
    "    keep_fmp_asof_date=keep_fmp_asof_date,\n",
    ")\n",
    "\n",
    "df = df.sort_values([\"symbol\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "# Base features: todo lo numérico excepto columnas \"id\"\n",
    "non_feature_cols = {\"symbol\", \"timestamp\", \"timeframe\"}\n",
    "base_feature_cols = [c for c in df.columns if c not in non_feature_cols]\n",
    "\n",
    "# Mantén solo numéricas (evita columnas tipo fecha/string si existieran)\n",
    "base_feature_cols = [c for c in base_feature_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "\n",
    "# (Opcional) normaliza bools a int\n",
    "for c in base_feature_cols:\n",
    "    if df[c].dtype == bool:\n",
    "        df[c] = df[c].astype(np.int8)\n",
    "\n",
    "print(\"df shape:\", df.shape)\n",
    "print(\"n base_feature_cols:\", len(base_feature_cols))\n",
    "print(\"example cols:\", base_feature_cols[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33822454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_predict_best_iteration(model: XGBRegressor, X: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predice respetando best_iteration si hubo early stopping.\n",
    "    Soporta versiones nuevas y viejas de xgboost.\n",
    "    \"\"\"\n",
    "    # xgboost >= 1.6: iteration_range\n",
    "    try:\n",
    "        bi = getattr(model, \"best_iteration\", None)\n",
    "        if bi is not None:\n",
    "            return model.predict(X, iteration_range=(0, int(bi) + 1))\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "    # xgboost <= 1.5: ntree_limit con best_ntree_limit\n",
    "    bntl = getattr(model, \"best_ntree_limit\", None)\n",
    "    if bntl is not None:\n",
    "        return model.predict(X, ntree_limit=int(bntl))\n",
    "\n",
    "    return model.predict(X)\n",
    "\n",
    "def pick_score(metrics_val: dict) -> float:\n",
    "    \"\"\"\n",
    "    Score para seleccionar hiperparámetros:\n",
    "    prioridad RankIC cross-sectional diario, fallback spread sharpe, etc.\n",
    "    \"\"\"\n",
    "    # Ajusta estas keys si tu eval_regression_extended usa nombres distintos\n",
    "    for k in [\"DailyRankIC_mean\", \"QuantileSpread_sharpe\", \"SpearmanCorr(RankIC)\", \"PearsonCorr(IC)\"]:\n",
    "        v = metrics_val.get(k, None)\n",
    "        if v is not None and np.isfinite(v):\n",
    "            return float(v)\n",
    "    return float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a646fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "HORIZON = 60 | lookback = 252\n",
      "Shapes: train (110416, 513) val (25912, 513) test (32664, 513) | cut: 2022-09-19 2024-03-19\n",
      "  [01/8] score= 0.052216 params={'colsample_bytree': 0.8, 'learning_rate': 0.03, 'max_depth': 4, 'min_child_weight': 1, 'reg_lambda': 1.0, 'subsample': 0.8} best_it=4\n",
      "  [02/8] score= 0.052218 params={'colsample_bytree': 0.8, 'learning_rate': 0.03, 'max_depth': 4, 'min_child_weight': 5, 'reg_lambda': 1.0, 'subsample': 0.8} best_it=4\n",
      "  [03/8] score=-0.038203 params={'colsample_bytree': 0.8, 'learning_rate': 0.03, 'max_depth': 6, 'min_child_weight': 1, 'reg_lambda': 1.0, 'subsample': 0.8} best_it=9\n",
      "  [04/8] score= 0.066990 params={'colsample_bytree': 0.8, 'learning_rate': 0.03, 'max_depth': 6, 'min_child_weight': 5, 'reg_lambda': 1.0, 'subsample': 0.8} best_it=1\n",
      "  [05/8] score= 0.080789 params={'colsample_bytree': 0.8, 'learning_rate': 0.07, 'max_depth': 4, 'min_child_weight': 1, 'reg_lambda': 1.0, 'subsample': 0.8} best_it=1\n",
      "  [06/8] score= 0.080790 params={'colsample_bytree': 0.8, 'learning_rate': 0.07, 'max_depth': 4, 'min_child_weight': 5, 'reg_lambda': 1.0, 'subsample': 0.8} best_it=1\n",
      "  [07/8] score=-0.026033 params={'colsample_bytree': 0.8, 'learning_rate': 0.07, 'max_depth': 6, 'min_child_weight': 1, 'reg_lambda': 1.0, 'subsample': 0.8} best_it=4\n",
      "  [08/8] score= 0.056323 params={'colsample_bytree': 0.8, 'learning_rate': 0.07, 'max_depth': 6, 'min_child_weight': 5, 'reg_lambda': 1.0, 'subsample': 0.8} best_it=1\n",
      "\n",
      "Saved: runs/xgb_1Day_lb252_h60_indicators1_econ0_fmp0_md4_lr0.07_mcw5_seed42\n",
      "VAL key metrics (si existen): DailyRankIC_mean= 0.08079038193900684 QuantileSpread_sharpe= 5.048823507563061\n",
      "TEST key metrics (si existen): DailyRankIC_mean= 0.06155858816065501 QuantileSpread_sharpe= 6.9966496260569\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_dir</th>\n",
       "      <th>horizon</th>\n",
       "      <th>lookback</th>\n",
       "      <th>val_score</th>\n",
       "      <th>test_rankic</th>\n",
       "      <th>test_spread_sharpe</th>\n",
       "      <th>params</th>\n",
       "      <th>best_iteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>runs/xgb_1Day_lb252_h60_indicators1_econ0_fmp0...</td>\n",
       "      <td>60</td>\n",
       "      <td>252</td>\n",
       "      <td>0.08079</td>\n",
       "      <td>0.061559</td>\n",
       "      <td>6.99665</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'learning_rate': 0.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             run_dir  horizon  lookback  \\\n",
       "0  runs/xgb_1Day_lb252_h60_indicators1_econ0_fmp0...       60       252   \n",
       "\n",
       "   val_score  test_rankic  test_spread_sharpe  \\\n",
       "0    0.08079     0.061559             6.99665   \n",
       "\n",
       "                                              params  best_iteration  \n",
       "0  {'colsample_bytree': 0.8, 'learning_rate': 0.0...               1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Hiperparámetros: grid barato y razonable\n",
    "# -----------------------\n",
    "xgb_base = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_estimators=5000,\n",
    "    # learning_rate=0.05,\n",
    "    # max_depth=6,\n",
    "    # subsample=0.8,\n",
    "    # colsample_bytree=0.8,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\",\n",
    "\n",
    "    # <-- aquí (no en fit)\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=200,\n",
    ")\n",
    "\n",
    "param_grid = list(ParameterGrid({\n",
    "    \"max_depth\": [4, 6],\n",
    "    \"learning_rate\": [0.03, 0.07],\n",
    "    \"min_child_weight\": [1, 5],\n",
    "    \"subsample\": [0.8],\n",
    "    \"colsample_bytree\": [0.8],\n",
    "    \"reg_lambda\": [1.0],\n",
    "    # \"gamma\": [0.0, 1.0],       # (opcional) más regularización\n",
    "}))\n",
    "\n",
    "# -----------------------\n",
    "# Entrenamiento por horizonte\n",
    "# -----------------------\n",
    "results = []\n",
    "saved_run_dirs = []\n",
    "\n",
    "for horizon in horizons:\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(f\"HORIZON = {horizon} | lookback = {lookback}\")\n",
    "\n",
    "    X, y, meta = build_supervised_dataset(\n",
    "        df=df,\n",
    "        feature_cols=base_feature_cols,\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        price_col=\"close\",\n",
    "        group_col=\"symbol\",\n",
    "        timestamp_col=\"timestamp\",\n",
    "        lags_by_feature=lags_by_feature,\n",
    "        default_lags=default_lags,\n",
    "    )\n",
    "\n",
    "    # Meta datetime\n",
    "    meta = meta.copy()\n",
    "    meta[\"timestamp\"] = pd.to_datetime(meta[\"timestamp\"])\n",
    "    meta[\"target_timestamp\"] = pd.to_datetime(meta[\"target_timestamp\"])\n",
    "\n",
    "    # Limpieza básica\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # Split por target_timestamp (sin leakage por horizon)\n",
    "    tr_mask, va_mask, te_mask, train_end, val_end= time_split_masks(meta, train_frac=0.65, val_frac=0.17)\n",
    "\n",
    "    X_train, y_train, meta_train = X.loc[tr_mask], y.loc[tr_mask], meta.loc[tr_mask]\n",
    "    X_val, y_val, meta_val = X.loc[va_mask], y.loc[va_mask], meta.loc[va_mask]\n",
    "    X_test, y_test, meta_test = X.loc[te_mask], y.loc[te_mask], meta.loc[te_mask]\n",
    "\n",
    "    \n",
    "\n",
    "    # (Opcional) float32 para memoria/velocidad\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_val = X_val.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    feature_names = list(X_train.columns)\n",
    "\n",
    "    print(\"Shapes:\",\n",
    "          \"train\", X_train.shape,\n",
    "          \"val\", X_val.shape,\n",
    "          \"test\", X_test.shape,\n",
    "          \"| cut:\", train_end.date(), val_end.date())\n",
    "\n",
    "    best = None  # dict con modelo, params, metrics\n",
    "    for i, params in enumerate(param_grid):\n",
    "        model = XGBRegressor(**xgb_base, **params)\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            # eval_metric=\"rmse\",\n",
    "            verbose=False,\n",
    "            # early_stopping_rounds=200,\n",
    "        )\n",
    "\n",
    "        y_val_pred = xgb_predict_best_iteration(model, X_val)\n",
    "        m_val = eval_regression_extended(\n",
    "            y_true=np.asarray(y_val, dtype=np.float64),\n",
    "            y_pred=np.asarray(y_val_pred, dtype=np.float64),\n",
    "            meta=meta_val,\n",
    "            time_col=\"timestamp\",\n",
    "            group_col=\"symbol\",\n",
    "            quantile=0.1,\n",
    "            periods_per_year=252,\n",
    "        )\n",
    "        score = pick_score(m_val)\n",
    "\n",
    "        row = dict(\n",
    "            horizon=horizon,\n",
    "            lookback=lookback,\n",
    "            grid_i=i,\n",
    "            score=score,\n",
    "            params=params,\n",
    "            best_iteration=getattr(model, \"best_iteration\", None),\n",
    "            val_metrics=m_val,\n",
    "        )\n",
    "\n",
    "        if (best is None) or (np.isfinite(score) and score > best[\"score\"]):\n",
    "            best = row\n",
    "\n",
    "        print(f\"  [{i+1:02d}/{len(param_grid)}] score={score: .6f} params={params} best_it={row['best_iteration']}\")\n",
    "\n",
    "    if best is None:\n",
    "        raise RuntimeError(\"No se pudo seleccionar un modelo (best=None).\")\n",
    "\n",
    "    # Re-entrena NO; usamos el mejor ya entrenado. Para eso necesitamos volver a entrenar con sus params\n",
    "    # (porque en el loop no guardamos el objeto modelo).\n",
    "    best_params = best[\"params\"]\n",
    "    best_model = XGBRegressor(**xgb_base, **best_params)\n",
    "    best_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        # eval_metric=\"rmse\",\n",
    "        verbose=False,\n",
    "        # early_stopping_rounds=200,\n",
    "    )\n",
    "\n",
    "    # Eval final val/test\n",
    "    y_val_pred = xgb_predict_best_iteration(best_model, X_val)\n",
    "    y_test_pred = xgb_predict_best_iteration(best_model, X_test)\n",
    "\n",
    "    metrics_val = eval_regression_extended(\n",
    "        y_true=np.asarray(y_val, dtype=np.float64),\n",
    "        y_pred=np.asarray(y_val_pred, dtype=np.float64),\n",
    "        meta=meta_val,\n",
    "        time_col=\"timestamp\",\n",
    "        group_col=\"symbol\",\n",
    "        quantile=0.1,\n",
    "        periods_per_year=252,\n",
    "    )\n",
    "    metrics_test = eval_regression_extended(\n",
    "        y_true=np.asarray(y_test, dtype=np.float64),\n",
    "        y_pred=np.asarray(y_test_pred, dtype=np.float64),\n",
    "        meta=meta_test,\n",
    "        time_col=\"timestamp\",\n",
    "        group_col=\"symbol\",\n",
    "        quantile=0.1,\n",
    "        periods_per_year=252,\n",
    "    )\n",
    "\n",
    "    # pred dfs\n",
    "    pred_df_val = meta_val.copy()\n",
    "    pred_df_val[\"y_true\"] = np.asarray(y_val, dtype=np.float64)\n",
    "    pred_df_val[\"y_pred\"] = np.asarray(y_val_pred, dtype=np.float64)\n",
    "    pred_df_val[\"error\"] = pred_df_val[\"y_pred\"] - pred_df_val[\"y_true\"]\n",
    "\n",
    "    pred_df_test = meta_test.copy()\n",
    "    pred_df_test[\"y_true\"] = np.asarray(y_test, dtype=np.float64)\n",
    "    pred_df_test[\"y_pred\"] = np.asarray(y_test_pred, dtype=np.float64)\n",
    "    pred_df_test[\"error\"] = pred_df_test[\"y_pred\"] - pred_df_test[\"y_true\"]\n",
    "\n",
    "    # Pipeline sklearn (sin preprocesado extra)\n",
    "    pipeline = Pipeline([(\"model\", best_model)])\n",
    "\n",
    "    # Flags (igual que en tu MLP)\n",
    "    ind_flag = int(bool(include_indicators))\n",
    "    econ_flag = int(bool(include_economic_indicators and econ_indicator_names is not None and len(econ_indicator_names) > 0))\n",
    "    fmp_flag = int(bool(include_fmp))\n",
    "\n",
    "    # Run dir (incluye algunos params clave)\n",
    "    run_dir = (\n",
    "        f\"runs/xgb_{timeframe}_lb{lookback}_h{horizon}\"\n",
    "        f\"_indicators{ind_flag}_econ{econ_flag}_fmp{fmp_flag}\"\n",
    "        f\"_md{best_params['max_depth']}_lr{best_params['learning_rate']}\"\n",
    "        f\"_mcw{best_params['min_child_weight']}_seed{xgb_base['random_state']}\"\n",
    "    )\n",
    "\n",
    "    config = {\n",
    "        \"model\": \"xgboost_regressor\",\n",
    "        \"timeframe\": timeframe,\n",
    "        \"symbols\": list(symbols),\n",
    "        \"lookback\": int(lookback),\n",
    "        \"horizon\": int(horizon),\n",
    "\n",
    "        \"base_feature_cols\": list(base_feature_cols),\n",
    "        \"include_indicators\": bool(include_indicators),\n",
    "        \"indicators_used\": indicator_names if include_indicators else [],\n",
    "        \"include_economic_indicators\": bool(include_economic_indicators),\n",
    "        \"econ_indicator_names\": econ_indicator_names if include_economic_indicators else [],\n",
    "        \"include_fmp\": bool(include_fmp),\n",
    "        \"fmp_prefix\": \"fmp\",\n",
    "        \"fmp_feature_names\": fmp_feature_names if include_fmp else [],\n",
    "        \"keep_fmp_asof_date\": bool(keep_fmp_asof_date),\n",
    "\n",
    "        \"lags_by_feature\": lags_by_feature,\n",
    "        \"default_lags\": default_lags,\n",
    "\n",
    "        \"xgb_base\": xgb_base,\n",
    "        \"xgb_params\": best_params,\n",
    "        \"xgb_best_iteration\": getattr(best_model, \"best_iteration\", None),\n",
    "\n",
    "        \"split\": {\n",
    "            \"train_frac\": 0.70,\n",
    "            \"val_frac\": 0.15,\n",
    "            \"train_end_target_timestamp\": str(pd.Timestamp(train_end)),\n",
    "            \"val_end_target_timestamp\": str(pd.Timestamp(val_end)),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    metrics = {\"val\": metrics_val, \"test\": metrics_test}\n",
    "\n",
    "    # Guardar artefacto\n",
    "    saved_path = save_sklearn_pipeline_artifact(\n",
    "        run_dir=run_dir,\n",
    "        pipeline=pipeline,\n",
    "        config=config,\n",
    "        metrics=metrics,\n",
    "        feature_names=feature_names,\n",
    "        pred_df_val=pred_df_val,\n",
    "        pred_df_test=pred_df_test,\n",
    "        extra={\n",
    "            \"note\": \"XGBoost baseline seleccionado por métricas en validation\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"\\nSaved:\", saved_path)\n",
    "    print(\"VAL key metrics (si existen):\",\n",
    "          \"DailyRankIC_mean=\", metrics_val.get(\"DailyRankIC_mean\"),\n",
    "          \"QuantileSpread_sharpe=\", metrics_val.get(\"QuantileSpread_sharpe\"))\n",
    "\n",
    "    print(\"TEST key metrics (si existen):\",\n",
    "          \"DailyRankIC_mean=\", metrics_test.get(\"DailyRankIC_mean\"),\n",
    "          \"QuantileSpread_sharpe=\", metrics_test.get(\"QuantileSpread_sharpe\"))\n",
    "\n",
    "    results.append({\n",
    "        \"run_dir\": saved_path,\n",
    "        \"horizon\": horizon,\n",
    "        \"lookback\": lookback,\n",
    "        \"val_score\": pick_score(metrics_val),\n",
    "        \"test_rankic\": metrics_test.get(\"DailyRankIC_mean\", np.nan),\n",
    "        \"test_spread_sharpe\": metrics_test.get(\"QuantileSpread_sharpe\", np.nan),\n",
    "        \"params\": best_params,\n",
    "        \"best_iteration\": getattr(best_model, \"best_iteration\", None),\n",
    "    })\n",
    "    saved_run_dirs.append(saved_path)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values([\"horizon\"])\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f467a1",
   "metadata": {},
   "source": [
    "# rerunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82938950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking: runs/xgb_1Day_lb252_h60_indicators1_econ0_fmp0_md4_lr0.07_mcw5_seed42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAE': 0.11758746108858927,\n",
       " 'MedianAE': 0.07705190777778625,\n",
       " 'RMSE': 0.20587868658505612,\n",
       " 'R2': 0.0019012529271140721,\n",
       " 'HitRate(sign)': 0.6215021199273167,\n",
       " 'HitRate(sign,deadzone)': 0.6215021199273167,\n",
       " 'PearsonCorr(IC)': 0.06877866639215588,\n",
       " 'SpearmanCorr(RankIC)': 0.14000297471034687,\n",
       " 'AUC(Sign)': 0.5174542535518284,\n",
       " 'N': 16510,\n",
       " 'N_deadzone': 16510,\n",
       " 'DailyIC_mean': 0.04941676579944929,\n",
       " 'DailyIC_std': 0.17053508862943642,\n",
       " 'DailyIC_tstat': 3.898520791417481,\n",
       " 'DailyIC_frac_pos': 0.6243093922651933,\n",
       " 'DailyIC_N': 181,\n",
       " 'DailyRankIC_mean': 0.07584926982328118,\n",
       " 'DailyRankIC_std': 0.17452842773564942,\n",
       " 'DailyRankIC_tstat': 5.846884508654975,\n",
       " 'DailyRankIC_frac_pos': 0.6685082872928176,\n",
       " 'DailyRankIC_N': 181,\n",
       " 'QuantileSpread_mean': 0.0592149688116876,\n",
       " 'QuantileSpread_std': 0.07866485084356947,\n",
       " 'QuantileSpread_sharpe': 11.949536268470148,\n",
       " 'QuantileSpread_N': 202,\n",
       " 'Conformal_qhat(alpha=0.1)': nan,\n",
       " 'Conformal_coverage(alpha=0.1)': nan,\n",
       " 'Conformal_avg_width(alpha=0.1)': nan,\n",
       " 'Conformal_width_over_std(alpha=0.1)': nan}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Toma uno de los run_dir guardados (por ejemplo el de horizon=60)\n",
    "run_dir = 'runs/xgb_1Day_lb252_h60_indicators1_econ0_fmp0_md4_lr0.07_mcw5_seed42'\n",
    "print(\"Checking:\", run_dir)\n",
    "\n",
    "pred_df, _meta = predict_artifact_to_compare(\n",
    "    run_dir,\n",
    "    conn=conn,\n",
    "    enforce_timeframe_match=True,\n",
    "    start='01-01-2024'\n",
    ")\n",
    "\n",
    "# Evalua de nuevo (debería coincidir con lo guardado si start/end no cambian)\n",
    "m = eval_regression_extended(\n",
    "    y_true=pred_df[\"y_true\"].to_numpy(dtype=np.float64),\n",
    "    y_pred=pred_df[\"y_pred\"].to_numpy(dtype=np.float64),\n",
    "    meta=pred_df[[\"symbol\", \"timestamp\", \"target_timestamp\"]],\n",
    "    time_col=\"timestamp\",\n",
    "    group_col=\"symbol\",\n",
    "    quantile=0.1,\n",
    "    periods_per_year=252,\n",
    ")\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffc7385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankIC: 0.021510059504387728 Spread: -0.0006339212595197327\n",
      "RankIC(flipped): 0.021510059504387728 Spread(flipped): -0.0006339212595197327\n"
     ]
    }
   ],
   "source": [
    "m = eval_regression_extended(pred_df['y_true'], pred_df['y_pred'], meta=_meta)\n",
    "m_flip = eval_regression_extended(pred_df['y_true'], pred_df['y_pred'], meta=_meta)\n",
    "\n",
    "print(\"RankIC:\", m.get(\"DailyRankIC_mean\"), \"Spread:\", m.get(\"QuantileSpread_mean\"))\n",
    "print(\"RankIC(flipped):\", m_flip.get(\"DailyRankIC_mean\"), \"Spread(flipped):\", m_flip.get(\"QuantileSpread_mean\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "investenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
