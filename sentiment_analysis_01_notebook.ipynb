{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf80674",
   "metadata": {},
   "source": [
    "# 02_openai_sentiment\n",
    "\n",
    "**Objetivo:** clasificar sentimiento (positivo/neutral/negativo) con **OpenAI Responses API** usando **Structured Outputs (JSON Schema)** para maximizar fiabilidad.\n",
    "\n",
    "**¬øPor qu√© Structured Outputs y no ‚Äútexto libre‚Äù?**\n",
    "- El modelo **debe** producir un JSON que **cumple** un **JSON Schema** ‚Üí reduce parsing fr√°gil y errores de formato.  \n",
    "- Menos *prompt brittleness*: definimos contrato de salida (tipos, enum, campos obligatorios).  \n",
    "- Integraci√≥n directa con validadores locales y BD.\n",
    "\n",
    "**Flujo (diagrama ASCII)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd8ad27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from http.client import REQUEST_TIMEOUT\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import hashlib\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple, Iterable\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "from jsonschema import Draft202012Validator, validate, ValidationError\n",
    "from tabulate import tabulate\n",
    "\n",
    "# OpenAI SDK\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception as e:\n",
    "    raise RuntimeError('library openai needed')\n",
    "\n",
    "# LiteLLM\n",
    "try:\n",
    "    import litellm\n",
    "    LITELLM_AVAILABLE = True\n",
    "except Exception:\n",
    "    LITELLM_AVAILABLE = False\n",
    "\n",
    "OPENAL_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "if not OPENAL_API_KEY:\n",
    "    print('OPENAI_API_KEY missing')\n",
    "\n",
    "OPENAI_MODEL = 'gpt-5-nano'\n",
    "TEMPERATURE = 1.0\n",
    "MAX_OUTPUT_TOKENS = 128\n",
    "REQUEST_TIMEOUT_ = 30 # seconds\n",
    "MAX_CONCURRENCY = 4\n",
    "BATCH_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "017b6169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_scripts.LLM_analysis.preprocess_store_database import resolve_db_path, get_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c659a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Schema para Structured Outputs\n",
    "SENTIMENT_JSON_SCHEMA = {\n",
    "    \"name\": \"sentiment_schema\",\n",
    "    \"schema\": {\n",
    "        \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "        \"type\": \"object\",\n",
    "        \"additionalProperties\": False,\n",
    "        \"properties\": {\n",
    "            \"sentiment\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"positive\", \"neutral\", \"negative\"]\n",
    "            },\n",
    "            \"confidence\": {\n",
    "                \"type\": \"number\",\n",
    "                \"minimum\": 1.0,\n",
    "                \"maximum\": 10.0\n",
    "            },\n",
    "            \"relevance\" :{\n",
    "                \"type\": \"number\",\n",
    "                \"minimum\": 1.0,\n",
    "                \"maximum\": 10.0\n",
    "            },\n",
    "            \"explanation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"minLength\": 0,\n",
    "                \"maxLength\": 512\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"sentiment\", \"confidence\"]\n",
    "    },\n",
    "    # strict=True hace que el modelo **tenga** que cumplir el schema\n",
    "    \"strict\": True\n",
    "}\n",
    "\n",
    "# Validador local (jsonschema)\n",
    "SENTIMENT_VALIDATOR = Draft202012Validator(SENTIMENT_JSON_SCHEMA[\"schema\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9dd6e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "COST_TABLE_USD_PER_1K = {\n",
    "    # Rellena con precios actuales si los conoces (input/output)\n",
    "    # \"gpt-4o-mini\": {\"input\": 0.0, \"output\": 0.0}\n",
    "    'gpt-5-nano': {'input':0.05 / 1000,\t'output': 0.40/1000}\n",
    "}\n",
    "\n",
    "def get_cost_per_1k(model: str) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Devuelve (precio_input, precio_output) USD por 1k tokens.\n",
    "    Si no est√° configurado, asume 0.0 para evitar sorpresas.\n",
    "    \"\"\"\n",
    "    entry = COST_TABLE_USD_PER_1K.get(model, {})\n",
    "    return (float(entry.get(\"input\", 0.0)), float(entry.get(\"output\", 0.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6918e756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_hash(text: str) -> str:\n",
    "    s = (text or \"\").strip()\n",
    "    return hashlib.blake2b(s.encode(\"utf-8\"), digest_size=16).hexdigest()\n",
    "\n",
    "def validate_or_raise(payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Valida contra el JSON Schema. Lanza ValidationError si no cumple.\n",
    "    \"\"\"\n",
    "    SENTIMENT_VALIDATOR.validate(payload)\n",
    "    return payload\n",
    "\n",
    "def backoff_sleep(base: float, attempt: int, max_sleep: float = 60.0) -> None:\n",
    "    \"\"\"\n",
    "    Exponential backoff with jitter.\n",
    "    \"\"\"\n",
    "    sleep_s = min(max_sleep, base * (2 ** attempt)) * (0.5 + 0.5 * os.urandom(1)[0] / 255)\n",
    "    time.sleep(sleep_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c58fba",
   "metadata": {},
   "source": [
    "# Actual prediction (LiteLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d138852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class OpenAIUsage(NamedTuple):\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "    total_tokens: int\n",
    "\n",
    "def _extract_usage(resp) -> OpenAIUsage:\n",
    "    \"\"\"\n",
    "    Extrae usage de la respuesta Responses API.\n",
    "    \"\"\"\n",
    "    u = getattr(resp, \"usage\", None) or {}\n",
    "    return OpenAIUsage(\n",
    "        input_tokens=int(u.get(\"input_tokens\", 0)),\n",
    "        output_tokens=int(u.get(\"output_tokens\", 0)),\n",
    "        total_tokens=int(u.get(\"total_tokens\", 0)),\n",
    "    )\n",
    "\n",
    "def _parse_json_from_response(resp) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    En Responses API puedes usar resp.output_text para obtener el texto agregado.\n",
    "    \"\"\"\n",
    "    text = getattr(resp, \"output_text\", None)\n",
    "    if not text:\n",
    "        # Fallback: navegar el √°rbol 'output'\n",
    "        out = getattr(resp, \"output\", None) or []\n",
    "        # Buscar el primer bloque de texto\n",
    "        for item in out:\n",
    "            # item.content es una lista de partes; buscar .text\n",
    "            content = getattr(item, \"content\", None) or []\n",
    "            for part in content:\n",
    "                if isinstance(part, dict) and part.get(\"type\") in (\"output_text\", \"input_text\", \"text\"):\n",
    "                    maybe = part.get(\"text\")\n",
    "                    if maybe:\n",
    "                        text = maybe\n",
    "                        break\n",
    "            if text:\n",
    "                break\n",
    "    if not text:\n",
    "        raise ValueError(\"No se pudo extraer texto del objeto de respuesta.\")\n",
    "    return json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d1a3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked(iterable: List[str], n: int) -> Iterable[List[str]]:\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "09ccee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LiteLLM helpers (corrige streaming y parsing) ===\n",
    "import json\n",
    "from typing import Tuple, Dict, Any, List, Optional\n",
    "\n",
    "try:\n",
    "    import litellm\n",
    "    LITELLM_AVAILABLE = True\n",
    "except Exception:\n",
    "    LITELLM_AVAILABLE = False\n",
    "\n",
    "from jsonschema import ValidationError\n",
    "\n",
    "def _parse_litellm_response(resp: Any) -> str:\n",
    "    \"\"\"\n",
    "    Extrae el contenido textual de la respuesta de LiteLLM.\n",
    "    Maneja dict, objetos ChatCompletion-like y evita el streaming wrapper.\n",
    "    \"\"\"\n",
    "    # 1) Respuesta dict estilo OpenAI\n",
    "    if isinstance(resp, dict):\n",
    "        try:\n",
    "            return resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except Exception:\n",
    "            pass\n",
    "    # 2) Objeto con atributo choices\n",
    "    if hasattr(resp, \"choices\"):\n",
    "        choice0 = resp.choices[0]\n",
    "        # OpenAI style\n",
    "        if hasattr(choice0, \"message\") and hasattr(choice0.message, \"content\"):\n",
    "            return choice0.message.content\n",
    "        # Algunas variantes usan dict-like\n",
    "        if isinstance(choice0, dict):\n",
    "            return choice0.get(\"message\", {}).get(\"content\", \"\")\n",
    "    # 3) Si accidentalmente vino en streaming (CustomStreamWrapper), pide non-stream\n",
    "    raise ValueError(\"Respuesta LiteLLM no parseable; aseg√∫rate de usar stream=False.\")\n",
    "\n",
    "def _extract_litellm_usage(resp: Any) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Extrae usage de una respuesta LiteLLM (si est√° disponible).\n",
    "    Retorna dict con prompt/completion/total tokens.\n",
    "    \"\"\"\n",
    "    usage = {\"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0}\n",
    "    # dict\n",
    "    if isinstance(resp, dict):\n",
    "        u = resp.get(\"usage\") or {}\n",
    "        usage[\"prompt_tokens\"] = int(u.get(\"prompt_tokens\", u.get(\"input_tokens\", 0)))\n",
    "        usage[\"completion_tokens\"] = int(u.get(\"completion_tokens\", u.get(\"output_tokens\", 0)))\n",
    "        usage[\"total_tokens\"] = int(u.get(\"total_tokens\", usage[\"prompt_tokens\"] + usage[\"completion_tokens\"]))\n",
    "        return usage\n",
    "    # objeto\n",
    "    u = getattr(resp, \"usage\", None)\n",
    "    if u:\n",
    "        usage[\"prompt_tokens\"] = int(getattr(u, \"prompt_tokens\", getattr(u, \"input_tokens\", 0)))\n",
    "        usage[\"completion_tokens\"] = int(getattr(u, \"completion_tokens\", getattr(u, \"output_tokens\", 0)))\n",
    "        usage[\"total_tokens\"] = int(getattr(u, \"total_tokens\", usage[\"prompt_tokens\"] + usage[\"completion_tokens\"]))\n",
    "    return usage\n",
    "\n",
    "def _classify_litellm_one(t: str,\n",
    "                          model_name: str,\n",
    "                          temperature: float,\n",
    "                          max_tokens: int,\n",
    "                          schema: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Clasifica un texto usando LiteLLM (no streaming) con Structured Outputs.\n",
    "    Devuelve (payload_json_validado, usage_dict).\n",
    "    \"\"\"\n",
    "    if not LITELLM_AVAILABLE:\n",
    "        raise RuntimeError(\"LiteLLM no est√° instalado. pip install litellm\")\n",
    "\n",
    "    system_instr = (\n",
    "        \"You are a financial sentiment classifier. \"\n",
    "        \"Return a JSON object that matches the provided schema exactly.\"\n",
    "    )\n",
    "\n",
    "    # üî¥ Forzamos stream=False para evitar CustomStreamWrapper\n",
    "\n",
    "    resp = litellm.completion(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_instr},\n",
    "            {\"role\": \"user\", \"content\": f\"Texto:\\n{t[:8000]}\\n\\nReturn JSON : sentiment (positive/neutral/negative), confidence [1, 10] (How confident are you about the sentiment selected), relevance [1, 10] (How relevant is this information in predicting related stock prices: 10 for instant buy/sell, 5 for barely informative), explanation (short).\"}\n",
    "        ],\n",
    "        # max_tokens=max_tokens,           # Chat Completions ‚Üí max_tokens (no max_output_tokens)\n",
    "        response_format={                 # Structured Outputs (si el backend lo soporta)\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": schema[\"name\"],\n",
    "                \"schema\": schema[\"schema\"],\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print('generated: ', resp)\n",
    "    content = _parse_litellm_response(resp)\n",
    "    try:\n",
    "        payload = json.loads(content)\n",
    "        validate_or_raise(payload)       # jsonschema local\n",
    "    except (json.JSONDecodeError, ValidationError):\n",
    "        payload = {\"sentiment\": \"neutral\", \"confidence\": 0.0, \"explanation\": \"fallback_invalid_json\"}\n",
    "\n",
    "    usage = _extract_litellm_usage(resp)\n",
    "    return payload, usage\n",
    "\n",
    "def classify_texts_litellm(texts: List[str],\n",
    "                           model_name: str = \"gpt-4o-mini\",\n",
    "                           max_concurrency: int = 4,\n",
    "                           batch_size: int = 20,\n",
    "                           temperature: float = 1.0,\n",
    "                           max_tokens: int = 128) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Concurrencia + batches con LiteLLM. Devuelve lista de dicts con sentimiento + usage.\n",
    "    \"\"\"\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    def _chunked(items, n):\n",
    "        for i in range(0, len(items), n):\n",
    "            yield items[i:i+n]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_concurrency) as pool:\n",
    "        for batch in _chunked(texts, batch_size):\n",
    "            futs = {\n",
    "                pool.submit(_classify_litellm_one, t, model_name, temperature, max_tokens, SENTIMENT_JSON_SCHEMA): t\n",
    "                for t in batch\n",
    "            }\n",
    "            for fut in as_completed(futs):\n",
    "                t = futs[fut]\n",
    "                payload, usage = fut.result()\n",
    "                results.append({\n",
    "                    \"text_hash\": content_hash(t),\n",
    "                    \"sentiment\": payload.get(\"sentiment\", \"neutral\"),\n",
    "                    \"confidence\": float(payload.get(\"confidence\", 0.0)),\n",
    "                    \"relevance\": float(payload.get(\"relevance\", 0)),\n",
    "                    \"explanation\": payload.get(\"explanation\", \"\"),\n",
    "                    \"usage_input_tokens\": int(usage.get(\"prompt_tokens\", 0)),\n",
    "                    \"usage_output_tokens\": int(usage.get(\"completion_tokens\", 0)),\n",
    "                    \"usage_total_tokens\": int(usage.get(\"total_tokens\", 0)),\n",
    "                    \"model\": model_name,\n",
    "                })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897206c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2925aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "\n",
    "PRED_TABLE = \"_02_sentiment_predictions\"\n",
    "\n",
    "SCHEMA_VERSION = \"sentiment_v1\"\n",
    "SCHEMA_VERSION_SENT = \"sentiment_v1\"\n",
    "SCHEMA_VERSION_THREAD = \"thread_v1\"\n",
    "\n",
    "DDL_PREDICTIONS = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {PRED_TABLE} (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    source TEXT NOT NULL,            -- 'reddit_post' | 'reddit_comment' | 'news' | ...\n",
    "    doc_id TEXT NOT NULL,            -- PK de la tabla origen\n",
    "    text_hash TEXT NOT NULL,\n",
    "    sentiment TEXT NOT NULL CHECK (sentiment IN ('positive','neutral','negative')),\n",
    "    confidence_10 INTEGER,\n",
    "    relevance_10 INTEGER,\n",
    "    explanation TEXT,\n",
    "    model TEXT NOT NULL,\n",
    "    schema_version TEXT NOT NULL DEFAULT '{SCHEMA_VERSION}',\n",
    "    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "    usage_input_tokens INTEGER,\n",
    "    usage_output_tokens INTEGER,\n",
    "    usage_total_tokens INTEGER,\n",
    "    cost_usd REAL,\n",
    "    UNIQUE (source, doc_id, model, schema_version)\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS idx_sent_texthash ON {PRED_TABLE}(text_hash);\n",
    "CREATE INDEX IF NOT EXISTS idx_sent_source_doc ON {PRED_TABLE}(source, doc_id);\n",
    "\"\"\"\n",
    "\n",
    "def init_predictions_schema(db_path: Optional[str] = None):\n",
    "    \n",
    "    con = get_connection(db_path)\n",
    "    try:\n",
    "        con.executescript(DDL_PREDICTIONS)\n",
    "        # intentamos agregar columnas si vienen de versi√≥n anterior\n",
    "        try: con.execute(f\"ALTER TABLE {PRED_TABLE} ADD COLUMN schema_version TEXT DEFAULT '{SCHEMA_VERSION}'\")\n",
    "        except sqlite3.OperationalError: pass\n",
    "        try: con.execute(f\"CREATE UNIQUE INDEX IF NOT EXISTS uq_sent_source_doc_model_ver ON {PRED_TABLE}(source, doc_id, model, schema_version)\")\n",
    "        except sqlite3.OperationalError: pass\n",
    "        con.commit()\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "def init_predictions_schema_extended(db_path: str | None = None):\n",
    "    con = get_connection(db_path)\n",
    "    try:\n",
    "        con.executescript(DDL_PREDICTIONS)\n",
    "        # Migraci√≥n suave (si vienes de versi√≥n anterior)\n",
    "        for col_def in [\n",
    "            \"ADD COLUMN confidence_10 INTEGER\",\n",
    "            \"ADD COLUMN relevance_10 INTEGER\",\n",
    "        ]:\n",
    "            try:\n",
    "                con.execute(f\"ALTER TABLE {PRED_TABLE} {col_def}\")\n",
    "            except sqlite3.OperationalError:\n",
    "                pass\n",
    "        try:\n",
    "            con.execute(f\"CREATE UNIQUE INDEX IF NOT EXISTS uq_pred ON {PRED_TABLE}(source, doc_id, model, schema_version)\")\n",
    "        except sqlite3.OperationalError:\n",
    "            pass\n",
    "        con.commit()\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "def write_predictions_to_sqlite(preds: List[Dict[str, Any]],\n",
    "                                source: str,\n",
    "                                id_list: List[str],\n",
    "                                db_path: Optional[str] = None,\n",
    "                                schema_version: str = SCHEMA_VERSION):\n",
    "    if len(id_list) != len(preds):\n",
    "        raise ValueError(\"id_list debe tener misma longitud que preds\")\n",
    "\n",
    "    price_in, price_out = get_cost_per_1k(OPENAI_MODEL)\n",
    "    rows = []\n",
    "    for i, p in enumerate(preds):\n",
    "        print(f'n {p}')\n",
    "        cost = (p[\"usage_input_tokens\"]/1000.0)*price_in + (p[\"usage_output_tokens\"]/1000.0)*price_out\n",
    "        rows.append((\n",
    "            source, id_list[i], p[\"text_hash\"], p[\"sentiment\"], int(p.get(\"confidence\", 0)), int(p.get(\"relevance\", 0)),\n",
    "            p.get(\"explanation\",\"\"), p.get(\"model\", OPENAI_MODEL), schema_version,\n",
    "            int(p.get(\"usage_input_tokens\",0)), int(p.get(\"usage_output_tokens\",0)), int(p.get(\"usage_total_tokens\",0)),\n",
    "            float(cost)\n",
    "        ))\n",
    "\n",
    "    con = get_connection(db_path)\n",
    "    try:\n",
    "        con.executemany(f\"\"\"\n",
    "        INSERT INTO {PRED_TABLE} (\n",
    "            source, doc_id, text_hash, sentiment, confidence_10, relevance_10, explanation,\n",
    "            model, schema_version,\n",
    "            usage_input_tokens, usage_output_tokens, usage_total_tokens, cost_usd\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ON CONFLICT(source, doc_id, model, schema_version) DO UPDATE SET\n",
    "            text_hash=excluded.text_hash,\n",
    "            sentiment=excluded.sentiment,\n",
    "            confidence_10=excluded.confidence_10,\n",
    "            relevance_10=excluded.relevance_10,\n",
    "            explanation=excluded.explanation,\n",
    "            usage_input_tokens=excluded.usage_input_tokens,\n",
    "            usage_output_tokens=excluded.usage_output_tokens,\n",
    "            usage_total_tokens=excluded.usage_total_tokens,\n",
    "            cost_usd=excluded.cost_usd,\n",
    "            created_at=CURRENT_TIMESTAMP;\n",
    "        \"\"\", rows)\n",
    "        con.commit()\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88693b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nsample_texts = [\\n    # Titulares\\n    \"NVDA se dispara 10% tras resultados; analistas suben precio objetivo.\",\\n    \"El mercado cae por temores de recesi√≥n, bancos lideran las p√©rdidas.\",\\n    \"Resultados de TSLA decepcionan: margen en m√≠nimos y gu√≠a recortada.\",\\n    \"La Fed mantendr√° tipos; se√±ales mixtas sobre inflaci√≥n subyacente.\",\\n    \"BTC supera los 80k USD; rotaci√≥n hacia cripto en riesgo-on.\",\\n    # Comentarios / tweets\\n    \"jajaja esto va to the moon üöÄüöÄ compr√© m√°s, diamond hands!\",\\n    \"vend√≠ en el m√≠nimo... qu√© desastre, nunca m√°s conf√≠o en ese CEO.\",\\n    \"meh, nada cambia con este reporte, neutral AF.\",\\n    \"qu√© robo esas comisiones, p√©simo broker üò§\",\\n    \"buen reporte, pero ya estaba descontado, poco recorrido.\"\\n]\\n\\ninit_predictions_schema(str(resolve_db_path))\\n\\npreds_llm = classify_texts_litellm(sample_texts, model_name=os.getenv(\"LITELLM_MODEL\", \"gpt-5-nano\"),\\n                                       max_concurrency=MAX_CONCURRENCY, batch_size=5)\\ndf2 = pd.DataFrame(preds_llm)\\n# Opci√≥n 1 (bonita)\\ntry:\\n    print(df2.to_markdown(index=False))\\nexcept Exception:\\n    # Opci√≥n 2 si no tienes tabulate/markdown\\n    print(df2.to_string(index=False))\\n\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChatGPT generated this\n",
    "\n",
    "'''\n",
    "\n",
    "sample_texts = [\n",
    "    # Titulares\n",
    "    \"NVDA se dispara 10% tras resultados; analistas suben precio objetivo.\",\n",
    "    \"El mercado cae por temores de recesi√≥n, bancos lideran las p√©rdidas.\",\n",
    "    \"Resultados de TSLA decepcionan: margen en m√≠nimos y gu√≠a recortada.\",\n",
    "    \"La Fed mantendr√° tipos; se√±ales mixtas sobre inflaci√≥n subyacente.\",\n",
    "    \"BTC supera los 80k USD; rotaci√≥n hacia cripto en riesgo-on.\",\n",
    "    # Comentarios / tweets\n",
    "    \"jajaja esto va to the moon üöÄüöÄ compr√© m√°s, diamond hands!\",\n",
    "    \"vend√≠ en el m√≠nimo... qu√© desastre, nunca m√°s conf√≠o en ese CEO.\",\n",
    "    \"meh, nada cambia con este reporte, neutral AF.\",\n",
    "    \"qu√© robo esas comisiones, p√©simo broker üò§\",\n",
    "    \"buen reporte, pero ya estaba descontado, poco recorrido.\"\n",
    "]\n",
    "\n",
    "init_predictions_schema(str(resolve_db_path))\n",
    "\n",
    "preds_llm = classify_texts_litellm(sample_texts, model_name=os.getenv(\"LITELLM_MODEL\", \"gpt-5-nano\"),\n",
    "                                       max_concurrency=MAX_CONCURRENCY, batch_size=5)\n",
    "df2 = pd.DataFrame(preds_llm)\n",
    "# Opci√≥n 1 (bonita)\n",
    "try:\n",
    "    print(df2.to_markdown(index=False))\n",
    "except Exception:\n",
    "    # Opci√≥n 2 si no tienes tabulate/markdown\n",
    "    print(df2.to_string(index=False))\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b3c56",
   "metadata": {},
   "source": [
    "## Thread builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d2abcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_thread_text(con: sqlite3.Connection,\n",
    "                      post_id: str,\n",
    "                      top_k_comments: int = 5,\n",
    "                      max_comment_chars: int = 600) -> tuple[str, list[str], dict]:\n",
    "    \"\"\"\n",
    "    Devuelve:\n",
    "      - thread_text (string para el prompt)\n",
    "      - included_comment_ids (lista de ids)\n",
    "      - meta (dict con score_post, num_comments, created_utc)\n",
    "\n",
    "    Usa:\n",
    "      - Post: reddit_posts + _01_reddit_posts_preprocessed (combined_llm)\n",
    "      - Comments: reddit_comments + _01_reddit_comments_preprocessed (body_llm)\n",
    "    \"\"\"\n",
    "    # Post base\n",
    "    row = con.execute(\"\"\"\n",
    "        SELECT p.post_id, p.subreddit, p.title, p.body, p.score, p.num_comments, p.created_utc,\n",
    "               pp.combined_llm\n",
    "        FROM reddit_posts p\n",
    "        JOIN _01_reddit_posts_preprocessed pp ON pp.post_id = p.post_id\n",
    "        WHERE p.post_id = ?;\n",
    "    \"\"\", (post_id,)).fetchone()\n",
    "    if not row:\n",
    "        raise ValueError(f\"post_id no encontrado: {post_id}\")\n",
    "\n",
    "    _, subreddit, title, body, score_post, num_comments, created_utc, post_llm = row\n",
    "    post_llm = (post_llm or \"\").strip()\n",
    "\n",
    "    # Top-K comments por score DESC, luego tiempo ASC\n",
    "    comments = con.execute(\"\"\"\n",
    "        SELECT c.comment_id, c.score, c.created_utc, cp.body_llm\n",
    "        FROM reddit_comments c\n",
    "        JOIN _01_reddit_comments_preprocessed cp ON cp.comment_id = c.comment_id\n",
    "        WHERE c.post_id = ?\n",
    "        ORDER BY c.score DESC, c.created_utc ASC\n",
    "        LIMIT ?;\n",
    "    \"\"\", (post_id, top_k_comments)).fetchall()\n",
    "\n",
    "    included_ids = []\n",
    "    comment_lines = []\n",
    "    for i, (cid, cscore, ctime, body_llm) in enumerate(comments, start=1):\n",
    "        included_ids.append(cid)\n",
    "        text = (body_llm or \"\").strip()\n",
    "        if max_comment_chars and len(text) > max_comment_chars:\n",
    "            text = text[:max_comment_chars] + \" ‚Ä¶\"\n",
    "        comment_lines.append(f\"{i}. [score={cscore}] {text}\")\n",
    "\n",
    "    # Armado del hilo en formato claro\n",
    "    header = f\"SUBREDDIT: {subreddit}\\nPOST_SCORE: {score_post}  NUM_COMMENTS: {num_comments}\\n\"\n",
    "    post_block = f\"POST:\\n{post_llm}\\n\"\n",
    "    if comment_lines:\n",
    "        comments_block = \"TOP_COMMENTS:\\n\" + \"\\n\".join(comment_lines) + \"\\n\"\n",
    "    else:\n",
    "        comments_block = \"TOP_COMMENTS:\\n(none)\\n\"\n",
    "\n",
    "    thread_text = header + post_block + comments_block\n",
    "\n",
    "    meta = {\"score_post\": score_post, \"num_comments\": num_comments, \"created_utc\": float(created_utc or 0.0)}\n",
    "    return thread_text, included_ids, meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78daa023",
   "metadata": {},
   "source": [
    "# Daily batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "32ed9b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_time_expr(source_table: str, time_col: str, alias: str | None = None) -> str:\n",
    "    tbl = alias or source_table\n",
    "    if source_table in (\"reddit_posts\", \"reddit_comments\") and time_col == \"created_utc\":\n",
    "        return f\"COALESCE({tbl}.{time_col}, 0.0)\"\n",
    "    if source_table == \"news_articles\" and time_col == \"published_at\":\n",
    "        # published_at puede ser NULL; fallback a fetch_date\n",
    "        return (\n",
    "            f\"COALESCE(\"\n",
    "            f\"CAST(strftime('%s', {tbl}.published_at) AS REAL), \"\n",
    "            f\"CAST(strftime('%s', {tbl}.fetch_date)   AS REAL), \"\n",
    "            f\"0.0)\"\n",
    "        )\n",
    "\n",
    "    return f\"COALESCE(CAST(strftime('%s', {tbl}.{time_col}) AS REAL), 0.0)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe4c1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_unscored_posts(con: sqlite3.Connection, limit: int, model: str, schema_version: str):\n",
    "    texpr = order_time_expr(\"reddit_posts\", \"created_utc\", alias=\"p\")\n",
    "    sql = f\"\"\"\n",
    "    SELECT p.post_id, pp.combined_llm AS text_llm, {texpr} AS t\n",
    "    FROM reddit_posts p\n",
    "    JOIN _01_reddit_posts_preprocessed pp ON pp.post_id = p.post_id\n",
    "    LEFT JOIN {PRED_TABLE} sp\n",
    "      ON sp.source='reddit_post' AND sp.doc_id=p.post_id\n",
    "     AND sp.model=? AND sp.schema_version=?\n",
    "    WHERE sp.doc_id IS NULL\n",
    "    ORDER BY t ASC, p.post_id ASC\n",
    "    LIMIT ?;\n",
    "    \"\"\"\n",
    "    return con.execute(sql, (model, schema_version, limit)).fetchall()\n",
    "\n",
    "def fetch_unscored_comments(con: sqlite3.Connection, limit: int, model: str, schema_version: str):\n",
    "    texpr = order_time_expr(\"reddit_comments\", \"created_utc\", alias=\"c\")\n",
    "    sql = f\"\"\"\n",
    "    SELECT c.comment_id, cp.body_llm AS text_llm, {texpr} AS t\n",
    "    FROM reddit_comments c\n",
    "    JOIN _01_reddit_comments_preprocessed cp ON cp.comment_id = c.comment_id\n",
    "    LEFT JOIN {PRED_TABLE} sp\n",
    "      ON sp.source='reddit_comment' AND sp.doc_id=c.comment_id\n",
    "     AND sp.model=? AND sp.schema_version=?\n",
    "    WHERE sp.doc_id IS NULL\n",
    "    ORDER BY t ASC, c.comment_id ASC\n",
    "    LIMIT ?;\n",
    "    \"\"\"\n",
    "    return con.execute(sql, (model, schema_version, limit)).fetchall()\n",
    "\n",
    "def fetch_unscored_news(con: sqlite3.Connection, limit: int, model: str, schema_version: str):\n",
    "    texpr = order_time_expr(\"news_articles\", \"published_at\", alias = 'n')\n",
    "    sql = f\"\"\"\n",
    "    SELECT n.url, np.combined_llm AS text_llm, {texpr} AS t\n",
    "    FROM news_articles n\n",
    "    JOIN _01_news_articles_preprocessed np ON np.url = n.url\n",
    "    LEFT JOIN {PRED_TABLE} sp\n",
    "      ON sp.source='news' AND sp.doc_id=n.url\n",
    "     AND sp.model=? AND sp.schema_version=?\n",
    "    WHERE sp.doc_id IS NULL\n",
    "    ORDER BY t ASC, n.url ASC\n",
    "    LIMIT ?;\n",
    "    \"\"\"\n",
    "    return con.execute(sql, (model, schema_version, limit)).fetchall()\n",
    "\n",
    "def fetch_unscored_threads(con: sqlite3.Connection, limit: int, model: str, schema_version: str):\n",
    "    \"\"\"\n",
    "    Devuelve posts que a√∫n no tienen predicci√≥n de 'reddit_thread' para (model, schema_version).\n",
    "    \"\"\"\n",
    "    texpr = order_time_expr(\"reddit_posts\", \"created_utc\", alias=\"p\")\n",
    "    sql = f\"\"\"\n",
    "    SELECT p.post_id, {texpr} AS t\n",
    "    FROM reddit_posts p\n",
    "    LEFT JOIN {PRED_TABLE} sp\n",
    "      ON sp.source='reddit_thread' AND sp.doc_id=p.post_id\n",
    "     AND sp.model=? AND sp.schema_version=?\n",
    "    WHERE sp.doc_id IS NULL\n",
    "    ORDER BY t ASC, p.post_id ASC\n",
    "    LIMIT ?;\n",
    "    \"\"\"\n",
    "    return con.execute(sql, (model, schema_version, limit)).fetchall()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4bcf7949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentiment_posts_batch_litellm(db_path: Optional[str] = None,\n",
    "                                          batch_size: int = 100,\n",
    "                                          model_name: str = 'gpt-4-nano',\n",
    "                                          max_concurrency: int = 4,\n",
    "                                          schema_version: str = SCHEMA_VERSION) -> int:\n",
    "    con = get_connection()\n",
    "    try:\n",
    "        rows = fetch_unscored_posts(con, batch_size, model_name, schema_version)\n",
    "\n",
    "    finally:\n",
    "        con.close()\n",
    "    \n",
    "    if not rows:\n",
    "        return 0\n",
    "    \n",
    "    ids = [r[0] for r in rows]\n",
    "    texts = [(r[1] or \"\").strip() for r in rows]\n",
    "    preds = classify_texts_litellm(texts, model_name=model_name, \n",
    "                                   max_concurrency=max_concurrency, batch_size=batch_size)\n",
    "    write_predictions_to_sqlite(preds, source='reddit_post', id_list=ids, db_path=db_path, schema_version=schema_version)\n",
    "    return len(preds)\n",
    "\n",
    "    \n",
    "def process_sentiment_comments_batch_litellm(db_path: Optional[str] = None,\n",
    "                                             batch_size: int = 200,\n",
    "                                             model_name: str = \"gpt-5-nano\",\n",
    "                                             max_concurrency: int = 4,\n",
    "                                             schema_version: str = SCHEMA_VERSION) -> int:\n",
    "    con = get_connection(db_path)\n",
    "    try:\n",
    "        rows = fetch_unscored_comments(con, batch_size, model_name, schema_version)\n",
    "    finally:\n",
    "        con.close()\n",
    "    if not rows:\n",
    "        return 0\n",
    "\n",
    "    ids = [r[0] for r in rows]\n",
    "    texts = [(r[1] or \"\").strip() for r in rows]\n",
    "    preds = classify_texts_litellm(texts, model_name=model_name,\n",
    "                                   max_concurrency=max_concurrency, batch_size=batch_size)\n",
    "    write_predictions_to_sqlite(preds, source=\"reddit_comment\", id_list=ids, db_path=db_path, schema_version=schema_version)\n",
    "    return len(preds)\n",
    "\n",
    "def process_sentiment_news_batch_litellm(db_path: Optional[str] = None,\n",
    "                                         batch_size: int = 200,\n",
    "                                         model_name: str = \"gpt-5-nano\",\n",
    "                                         max_concurrency: int = 4,\n",
    "                                         schema_version: str = SCHEMA_VERSION) -> int:\n",
    "    con = get_connection(db_path)\n",
    "    try:\n",
    "        rows = fetch_unscored_news(con, batch_size, model_name, schema_version)\n",
    "    finally:\n",
    "        con.close()\n",
    "    if not rows:\n",
    "        return 0\n",
    "\n",
    "    ids = [r[0] for r in rows]\n",
    "    texts = [(r[1] or \"\").strip() for r in rows]\n",
    "    preds = classify_texts_litellm(texts, model_name=model_name,\n",
    "                                   max_concurrency=max_concurrency, batch_size=batch_size)\n",
    "    write_predictions_to_sqlite(preds, source=\"news\", id_list=ids, db_path=db_path, schema_version=schema_version)\n",
    "    return len(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e9a48b",
   "metadata": {},
   "source": [
    "## Posts + comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "76b1200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree.ElementInclude import include\n",
    "\n",
    "\n",
    "def build_thread_text(con: sqlite3.Connection,\n",
    "                      post_id: str,\n",
    "                      top_k_comments: int = 5,\n",
    "                      max_comment_chars: int = 600) -> tuple[str, list[str], dict]:\n",
    "    '''\n",
    "    Returns:\n",
    "        - thread text (for the prompt)\n",
    "        - included_comment_ids (list of ids)\n",
    "        - meta (score_post, num_comments, created_utc)\n",
    "\n",
    "    '''\n",
    "    row = con.execute(\"\"\"\n",
    "                      SELECT p.post_id, p.subreddit, p.title, p.body, p.score, p.num_comments, p.created_utc,\n",
    "                      pp.combined_llm\n",
    "                      FROM reddit_posts p\n",
    "                      JOIN _01_reddit_posts_preprocessed pp ON pp.post_id = p.post_id\n",
    "                      WHERE p.post_id = ?;\n",
    "                      \"\"\", (post_id,)).fetchone()\n",
    "    \n",
    "    if not row:\n",
    "        raise ValueError(f'post_id not found: {post_id}')\n",
    "    \n",
    "    _, subreddit, title, body, score_post, num_comments, created_utc, post_llm = row\n",
    "    post_llm = (post_llm or \"\").strip()\n",
    "\n",
    "    # Top-K comments by score DESC and then time ASC\n",
    "    comments = con.execute(\"\"\"\n",
    "                            SELECT c.comment_id, c.score, c.created_utc, cp.body_llm \n",
    "                            FROM reddit_comments c\n",
    "                            JOIN _01_reddit_comments_preprocessed cp ON cp.comment_id = c.comment_id\n",
    "                            WHERE c.post_id = ?\n",
    "                            ORDER BY c.score DESC, c.created_utc ASC\n",
    "                            LIMIT ?;\n",
    "                           \"\"\", (post_id, top_k_comments)).fetchall()\n",
    "    \n",
    "    included_ids = []\n",
    "    comment_lines = []\n",
    "    for i, (cid, cscore, ctime, body_llm) in enumerate(comments, start=1):\n",
    "        included_ids.append(cid)\n",
    "        text = (body_llm or \"\").strip()\n",
    "        if max_comment_chars and len(text) > max_comment_chars:\n",
    "            text = text[:max_comment_chars] + \"...\"\n",
    "        comment_lines.append(f\"{i}. [score={cscore}] {text}\")\n",
    "\n",
    "    header = f\"SUBREDDIT: {subreddit}\\n\"\n",
    "    post_block = f\"POST:\\n{post_llm}\\n\"\n",
    "    if comment_lines:\n",
    "        comments_block = \"TOP_COMMENTS:\\n\" + \"\\n\".join(comment_lines) + \"\\n\"\n",
    "    else:\n",
    "        comments_block = \"TOP_COMMENTS:\\n(none)\\n\"\n",
    "\n",
    "    thread_text = header + post_block + comments_block\n",
    "\n",
    "    meta = {\"score_post\": score_post, \"num_comments\": num_comments, \"created_utc\": float(created_utc or 0.0)}\n",
    "    return thread_text, included_ids, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d02c9512",
   "metadata": {},
   "outputs": [],
   "source": [
    "THREAD_JSON_SCHEMA = {\n",
    "    \"name\": \"thread_sentiment_schema\",\n",
    "    \"schema\": {\n",
    "        \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "        \"type\": \"object\",\n",
    "        \"additionalProperties\": False,\n",
    "        \"properties\": {\n",
    "            \"sentiment\":       {\"type\": \"string\", \"enum\": [\"positive\", \"neutral\", \"negative\"]},\n",
    "            \"confidence\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 10},\n",
    "            \"relevance\":  {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 10},\n",
    "            \"explanation\": {\"type\": \"string\", \"minLength\": 0, \"maxLength\": 384}\n",
    "        },\n",
    "        \"required\": [\"sentiment\", \"confidence\", \"relevance\"]\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59cbe580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _classify_litellm_one_schema(t: str,\n",
    "                                 model_name: str,\n",
    "                                 temperature: float,\n",
    "                                 max_tokens: int,\n",
    "                                 schema: dict,\n",
    "                                 system_instr: str) -> tuple[dict, dict]:\n",
    "    # More generic version\n",
    "\n",
    "    resp = litellm.completion(\n",
    "        model = model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_instr},\n",
    "            {\"role\": \"user\", \"content\": t[:16000]},\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\"name\": schema[\"name\"], \"schema\": schema[\"schema\"]},\n",
    "        }\n",
    "    )\n",
    "    content = _parse_litellm_response(resp)\n",
    "    try:\n",
    "        payload = json.loads(content)\n",
    "        validate_or_raise(payload)\n",
    "    except Exception:\n",
    "        payload = {\"sentiment\": \"neutral\", \"confidence\": 1, \"relevance\": 1, \"explanation\": \"fallback_invalid_json\"}\n",
    "    usage = _extract_litellm_usage(resp)\n",
    "    return payload, usage\n",
    "\n",
    "def classify_threads_litellm(thread_texts: list[str],\n",
    "                             model_name: str = 'gpt-5-nano',\n",
    "                             max_concurrency: int = 4,\n",
    "                             batch_size: int = 20,\n",
    "                             temperature: float = 1.0,\n",
    "                             max_tokens: int = 128) -> list[dict]:\n",
    "    # Classifies list of thread texts\n",
    "    # Returns dicts with label/confidence/relevance + usage\n",
    "\n",
    "    system_instr = (\n",
    "        \"You are a financial sentiment rater. \"\n",
    "        \"Given a Reddit post and its top comments, return STRICT JSON matching the schema: \"\n",
    "        \"sentiment in {positive, neutral, negative}; \"\n",
    "        \"confidence integer 1..10 (how confident about the sentiment); \"\n",
    "        \"relevance integer 1..10 (10 = highly actionable for trading decisions, 5 = mildly informative). \"\n",
    "        \"Keep explanation short.\"\n",
    "    )\n",
    "\n",
    "    results: list[dict] = []\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    def _chunked(items, n):\n",
    "        for i in range(0, len(items), n):\n",
    "            yield items[i:i + n]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_concurrency) as pool:\n",
    "        for batch in _chunked(thread_texts, batch_size):\n",
    "            futs = {\n",
    "                pool.submit(_classify_litellm_one_schema, t, model_name, temperature, max_tokens, THREAD_JSON_SCHEMA, system_instr): t for t in batch\n",
    "\n",
    "\n",
    "            }\n",
    "            for fut in as_completed(futs):\n",
    "                t = futs[fut]\n",
    "                payload, usage = fut.result()\n",
    "                results.append({\n",
    "                    \"text_hash\": content_hash(t),\n",
    "                    \"sentiment\": payload.get(\"sentiment\", \"neutral\"),\n",
    "                    \"confidence\": int(payload.get(\"confidence\", 1)),\n",
    "                    \"relevance\": int(payload.get(\"relevance\", 1)),\n",
    "                    \"explanation\": payload.get(\"explanation\", \"\"),\n",
    "                    \"usage_input_tokens\": int(usage.get(\"prompt_tokens\", 0)),\n",
    "                    \"usage_output_tokens\": int(usage.get(\"completion_tokens\", 0)),\n",
    "                    \"model\": model_name,\n",
    "                })                \n",
    "    return results\n",
    "\n",
    "def process_thread_sentiment_batch_litellm(db_path: str | None = None,\n",
    "                                           batch_size: int = 100,\n",
    "                                           top_k_comments: int = 5,\n",
    "                                           model_name: str = \"gpt-4o-mini\",\n",
    "                                           max_concurrency: int = 6,\n",
    "                                           schema_version: str = SCHEMA_VERSION_THREAD) -> int:\n",
    "    \"\"\"\n",
    "    Arma hilos (post + top-K comentarios) y los clasifica con LiteLLM.\n",
    "    Reanuda autom√°ticamente gracias al NOT EXISTS en fetch_unscored_threads.\n",
    "    \"\"\"\n",
    "    con = get_connection(db_path)\n",
    "    try:\n",
    "        rows = fetch_unscored_threads(con, batch_size, model_name, schema_version)\n",
    "    finally:\n",
    "        con.close()\n",
    "    if not rows:\n",
    "        return 0\n",
    "\n",
    "    post_ids = [r[0] for r in rows]\n",
    "\n",
    "    # Construye los textos de hilo\n",
    "    thread_texts: list[str] = []\n",
    "    con = get_connection(db_path)\n",
    "    try:\n",
    "        for pid in post_ids:\n",
    "            ttext, comment_ids, meta = build_thread_text(con, pid, top_k_comments=top_k_comments)\n",
    "            # Puedes incluir el score del post en el encabezado (ya lo hace build_thread_text)\n",
    "            thread_texts.append(ttext)\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "    # Clasifica\n",
    "    preds = classify_threads_litellm(thread_texts, model_name=model_name,\n",
    "                                     max_concurrency=max_concurrency, batch_size=min(batch_size, 20))\n",
    "\n",
    "    # Escribe (source='reddit_thread')\n",
    "    write_predictions_to_sqlite(preds, source=\"reddit_thread\", id_list=post_ids,\n",
    "                                db_path=db_path, schema_version=schema_version)\n",
    "    return len(preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b66359b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_daily_sentiment_litellm(db_path: Optional[str] = None,\n",
    "                                per_source_cap: int = 2000,\n",
    "                                batch_size_posts: int = 200,\n",
    "                                batch_size_comments: int = 400,\n",
    "                                batch_size_news: int = 400,\n",
    "                                batch_size_threads: int = 50,\n",
    "                                top_k_comments: int = 5,\n",
    "                                model_name: str = \"gpt-4o-mini\",\n",
    "                                max_concurrency: int = 8,\n",
    "                                schema_version: str = SCHEMA_VERSION):\n",
    "    \"\"\"\n",
    "    Procesa en bucles de lotes hasta 'per_source_cap' por fuente.\n",
    "    Reanuda autom√°ticamente porque usa NOT EXISTS sobre la tabla de predicciones.\n",
    "    \"\"\"\n",
    "    init_predictions_schema(db_path)\n",
    "\n",
    "    processed = {\"posts\": 0, \"comments\": 0, \"news\": 0, \"threads\": 0}\n",
    "\n",
    "    while processed[\"posts\"] < per_source_cap:\n",
    "        n = process_sentiment_posts_batch_litellm(db_path, batch_size_posts, model_name, max_concurrency, schema_version)\n",
    "        print(f'Processed {n} posts')\n",
    "        if n == 0: break\n",
    "        processed[\"posts\"] += n\n",
    "        print(f\"[posts] +{n}  tot={processed['posts']}\")\n",
    "\n",
    "    while processed[\"comments\"] < per_source_cap:\n",
    "        n = process_sentiment_comments_batch_litellm(db_path, batch_size_comments, model_name, max_concurrency, schema_version)\n",
    "        print(f'Processed {n} comments')\n",
    "        if n == 0: break\n",
    "        processed[\"comments\"] += n\n",
    "        print(f\"[comments] +{n}  tot={processed['comments']}\")\n",
    "\n",
    "    while processed[\"news\"] < per_source_cap:\n",
    "        n = process_sentiment_news_batch_litellm(db_path, batch_size_news, model_name, max_concurrency, schema_version)\n",
    "        print(f'Processed {n} news')\n",
    "        if n == 0: break\n",
    "        processed[\"news\"] += n\n",
    "        print(f\"[news] +{n}  tot={processed['news']}\")\n",
    "\n",
    "    while processed[\"threads\"] < per_source_cap:\n",
    "        n = process_thread_sentiment_batch_litellm(db_path, batch_size_threads, top_k_comments,\n",
    "                                                   model_name, max_concurrency, SCHEMA_VERSION_THREAD)\n",
    "        if n == 0: break\n",
    "        processed[\"threads\"] += n\n",
    "        print(f\"[threads] +{n}  tot={processed['threads']}\")\n",
    "\n",
    "    print(\"[DONE sentiment]\", processed)\n",
    "    return processed\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "41df9178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated:  ModelResponse(id='chatcmpl-Cp4EU8yuMhYY1ToWeW6rUobVdWBIt', created=1766287034, model='gpt-5-nano-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='{\"sentiment\":\"positive\",\"confidence\":8,\"relevance\":9,\"explanation\":\"Optimistic AMD outlook due to breakout, leadership in chiplets and ROCm, potential non-NVDA demand, and favorable valuation metrics.\"}', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=445, prompt_tokens=292, total_tokens=737, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=384, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n",
      "generated:  ModelResponse(id='chatcmpl-Cp4EUFMj1PNltKnlhlxHT9LioQyKE', created=1766287034, model='gpt-5-nano-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='{\"sentiment\":\"negative\",\"confidence\":8,\"relevance\":6,\"explanation\":\"The text expresses worry about market conditions and Buffett\\'s overbought warning, indicating negative sentiment toward the market and potential impact on investment decisions.\"}', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=637, prompt_tokens=220, total_tokens=857, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=576, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n",
      "n {'text_hash': '11338d8a7c96908d63d79b342037ec0e', 'sentiment': 'positive', 'confidence': 8.0, 'relevance': 9.0, 'explanation': 'Optimistic AMD outlook due to breakout, leadership in chiplets and ROCm, potential non-NVDA demand, and favorable valuation metrics.', 'usage_input_tokens': 292, 'usage_output_tokens': 445, 'usage_total_tokens': 737, 'model': 'gpt-5-nano'}\n",
      "n {'text_hash': 'bc88e55162594956123d94e4e87454ea', 'sentiment': 'negative', 'confidence': 8.0, 'relevance': 6.0, 'explanation': \"The text expresses worry about market conditions and Buffett's overbought warning, indicating negative sentiment toward the market and potential impact on investment decisions.\", 'usage_input_tokens': 220, 'usage_output_tokens': 637, 'usage_total_tokens': 857, 'model': 'gpt-5-nano'}\n",
      "Processed 2 posts\n",
      "[posts] +2  tot=2\n",
      "generated:  ModelResponse(id='chatcmpl-Cp4EadSfrVHeG5QYtJRR3PnKWX5iW', created=1766287040, model='gpt-5-nano-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='{\"sentiment\":\"negative\",\"confidence\":8,\"relevance\":2,\"explanation\":\"Text conveys strong negative sentiment due to harm to a child; not informative for stock pricing.\"}', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=435, prompt_tokens=187, total_tokens=622, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=384, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n",
      "generated:  ModelResponse(id='chatcmpl-Cp4EaVCso7dX2iy2kDZqJwVgGCLyb', created=1766287040, model='gpt-5-nano-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='{\"sentiment\":\"negative\",\"confidence\":9,\"relevance\":1,\"explanation\":\"Strong negative sentiment toward a political figure; no financial content or stock-related information.\"}', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=817, prompt_tokens=199, total_tokens=1016, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=768, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n",
      "n {'text_hash': '55b7ed1b6710d65499fc183dcd11c4c4', 'sentiment': 'negative', 'confidence': 8.0, 'relevance': 2.0, 'explanation': 'Text conveys strong negative sentiment due to harm to a child; not informative for stock pricing.', 'usage_input_tokens': 187, 'usage_output_tokens': 435, 'usage_total_tokens': 622, 'model': 'gpt-5-nano'}\n",
      "n {'text_hash': 'b1d5c7bc7064f1cb7e11728fa9c3f61a', 'sentiment': 'negative', 'confidence': 9.0, 'relevance': 1.0, 'explanation': 'Strong negative sentiment toward a political figure; no financial content or stock-related information.', 'usage_input_tokens': 199, 'usage_output_tokens': 817, 'usage_total_tokens': 1016, 'model': 'gpt-5-nano'}\n",
      "Processed 2 comments\n",
      "[comments] +2  tot=2\n",
      "generated:  ModelResponse(id='chatcmpl-Cp4EgcHD0trMV1LmV7lCPP5grkCiL', created=1766287046, model='gpt-5-nano-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='{\"sentiment\":\"positive\",\"confidence\":8,\"relevance\":6,\"explanation\":\"A firm increased its NVIDIA stake by 6.1%, signalling positive sentiment or confidence in NVIDIA\\'s prospects, which could influence its stock.\"}', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=445, prompt_tokens=297, total_tokens=742, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=384, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n",
      "generated:  ModelResponse(id='chatcmpl-Cp4EgKQRzLIwxIKJGQQUJRYRRgw8q', created=1766287046, model='gpt-5-nano-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='{\"sentiment\":\"neutral\",\"confidence\":6,\"relevance\":5,\"explanation\":\"Neutral, factual report of an institutional investor increasing NVDA stake; no explicit price guidance or opinion.\"}', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=949, prompt_tokens=277, total_tokens=1226, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=896, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n",
      "n {'text_hash': 'd7baf64602742735d5f60877a110fe1c', 'sentiment': 'positive', 'confidence': 8.0, 'relevance': 6.0, 'explanation': \"A firm increased its NVIDIA stake by 6.1%, signalling positive sentiment or confidence in NVIDIA's prospects, which could influence its stock.\", 'usage_input_tokens': 297, 'usage_output_tokens': 445, 'usage_total_tokens': 742, 'model': 'gpt-5-nano'}\n",
      "n {'text_hash': '996c7ea5fed291c9290da603e9c8c0a5', 'sentiment': 'neutral', 'confidence': 6.0, 'relevance': 5.0, 'explanation': 'Neutral, factual report of an institutional investor increasing NVDA stake; no explicit price guidance or opinion.', 'usage_input_tokens': 277, 'usage_output_tokens': 949, 'usage_total_tokens': 1226, 'model': 'gpt-5-nano'}\n",
      "Processed 2 news\n",
      "[news] +2  tot=2\n",
      "n {'text_hash': '9534999eb6de7d321282c9f81edde532', 'sentiment': 'negative', 'confidence': 7, 'relevance': 8, 'explanation': 'Post highlights overvaluation and a contrarian sell signal; several top comments reinforce bearish sentiment despite some AI optimism.', 'usage_input_tokens': 447, 'usage_output_tokens': 952, 'model': 'gpt-5-nano'}\n",
      "n {'text_hash': '7b60cea095bbba81f484bff34ae98d25', 'sentiment': 'neutral', 'confidence': 7, 'relevance': 2, 'explanation': 'Neutral political news item with partisan comments; no clear or actionable financial market impact.', 'usage_input_tokens': 462, 'usage_output_tokens': 882, 'model': 'gpt-5-nano'}\n",
      "[threads] +2  tot=2\n",
      "[DONE sentiment] {'posts': 2, 'comments': 2, 'news': 2, 'threads': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'posts': 2, 'comments': 2, 'news': 2, 'threads': 2}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "run_daily_sentiment_litellm(per_source_cap= 2, batch_size_posts=2, batch_size_comments=2, batch_size_news=2, batch_size_threads=2, model_name=\"gpt-5-nano\", max_concurrency=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad9a07",
   "metadata": {},
   "source": [
    "# Vistas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f785f3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1713696",
   "metadata": {},
   "source": [
    "# Cost estimation (unfinished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc454a4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mavg_input_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(avg_in, \u001b[32m2\u001b[39m),\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mavg_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(avg_out, \u001b[32m2\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m         }\n\u001b[32m     19\u001b[39m     }\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Ejemplo (ajusta items/d√≠a):\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m estimate_daily_cost_from_sample(\u001b[43mpreds\u001b[49m, expected_items_per_day=\u001b[32m5000\u001b[39m, model=OPENAI_MODEL)\n",
      "\u001b[31mNameError\u001b[39m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "def estimate_daily_cost_from_sample(preds: List[Dict[str, Any]], expected_items_per_day: int, model: str = OPENAI_MODEL) -> Dict[str, Any]:\n",
    "    if not preds:\n",
    "        return {\"avg_input_tokens\":0, \"avg_output_tokens\":0, \"avg_total_tokens\":0, \"daily_cost_usd\":0.0}\n",
    "    avg_in = sum(p[\"usage_input_tokens\"] for p in preds) / len(preds)\n",
    "    avg_out = sum(p[\"usage_output_tokens\"] for p in preds) / len(preds)\n",
    "    price_in, price_out = get_cost_per_1k(model)\n",
    "    daily_cost = (avg_in * expected_items_per_day / 1000.0) * price_in + (avg_out * expected_items_per_day / 1000.0) * price_out\n",
    "    return {\n",
    "        \"avg_input_tokens\": round(avg_in, 2),\n",
    "        \"avg_output_tokens\": round(avg_out, 2),\n",
    "        \"avg_total_tokens\": round(avg_in + avg_out, 2),\n",
    "        \"daily_cost_usd\": round(daily_cost, 4),\n",
    "        \"assumptions\": {\n",
    "            \"items_per_day\": expected_items_per_day,\n",
    "            \"model\": model,\n",
    "            \"price_in_per_1k\": price_in,\n",
    "            \"price_out_per_1k\": price_out\n",
    "        }\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "investenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
